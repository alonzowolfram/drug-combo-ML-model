---
title: Machine-learning models for drug combination prediction
output:
  word_document: default
  pdf_document:
    fig_caption: yes
  html_document:
    fig_caption: yes
---

# Setup
```{r}
source("Scripts/setup.R")
```

# Machine-learning data preparation
IC50 data from https://www.cancerrxgene.org/downloads/bulk_download.
Drug-target data from https://www.cancerrxgene.org/downloads/drug_data. 
Cell-line metadata from: https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-3610/E-MTAB-3610.sdrf.txt (including the name of the .cel file for each cell line.)

## GDSC
### Drug-target data: descriptor generation
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# 1) Get the transcriptional effects.
# 1a) From TRRUST.
GDSC_drug_target_data_TRRUST_filename = filename_generator(data_dir, data_source = "GDSC", data_type = "Drugs", data_set = "TRRUST", extension = ".rds", additional_info = "target_data", processing_stage = "Processed")
GDSC_drug_target_data_TRRUST = readRDS(GDSC_drug_target_data_TRRUST_filename)

# 1b) From CTD (transcriptional).
# Naming the modified function "getTargets3," how creative. 
getTargets3 = function(drug) {
  # For drug "drug," get the targets. 
  print(paste0("Getting targets for drug ", drug, "."))
  
  # Create an empty table to be returned for drugs without entries.
  empty_table = data.frame(
    Drug = character(),
    Target = character(),
    Effect = character(),
    stringsAsFactors = F
  )
  
  # Load the chemical-gene interactions from CTD (saved as a CSV file in /Data/CTD/ALMANAC.)
  possibleError = tryCatch(
    {
      # GDSC_drug_cgixns_CTD_filtered should already be loaded in the global environment. 
      # Keep only the interactions for the drug "drug" and found in humans.
      regex = paste0("^", drug, "$")
      CTD_subset = GDSC_drug_cgixns_CTD_filtered[GDSC_drug_cgixns_CTD_filtered$OrganismID=="9606" & (GDSC_drug_cgixns_CTD_filtered$ChemicalName==drug | GDSC_drug_cgixns_CTD_filtered$Input==drug | base::grepl(regex, GDSC_drug_cgixns_CTD_filtered$Input, ignore.case = T) | base::grepl(regex, GDSC_drug_cgixns_CTD_filtered$ChemicalName, ignore.case = T)),,drop=F]
    }, error=function(cond) {
      message(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, ":"))
      message(cond)
    }
  )
  
  if(inherits(possibleError, "error") | !exists("CTD_subset")) {
    print(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, "."))
    return(empty_table)
  }
  
  else {
    if(nrow(CTD_subset) < 1) return(empty_table)
    
    # We want only the interactions that include "[chemical name] (and not an analog) ... results in decreased/increased action/expression of [X protein/mRNA]."
    # regex = paste0(drug, " (?!\\banalog\\b)(binds to and results in |results in(\\s| (de|in)creased [^\\s]+ of and results in ))(de|in)creased (activity|expression) of") # Yes, there should be no space between "(?!\\banalog\\b)" and "results."
    # Standardize the drug name. 
    drug = CTD_subset$ChemicalName
    drug = drug[complete.cases(drug)][1]
  
    regex1 = paste0(" (?!\\banalog\\b)") # This will be set to negative, because we do NOT want the analogs. Just the drug itself. 
    regex2 = "(the reaction \\[|susceptibility to|co-treated|modified form)" # This will be set to negative, because we do NOT want the other drug-chemical interactions that the drug in question affects OR direct gene effects affected by other drugs. Just the direct gene effects. We also don't want co-treatments (for right now.) Finally, we don't want the modified forms of proteins. 
    regex3 = "results in (de|in)creased (activity|expression) of" # This will be set to positive, because we want to make sure that this phrase is present. 
    regex4 = "\\[" # We do NOT want something like "Carboplatin results in increased activity of [NFKB1 protein binds to RELA protein]"
    regex5 = "mRNA" # This will be set to positive, because we only want transcriptional effects this time. 
    # We will use the regexPipes library to do this. 
    interactions = CTD_subset$Interaction %>% regexPipes::grep(drug, fixed = T, value = T) %>% regexPipes::grep(regex1, perl = T, value = T) %>% regexPipes::grep(regex2, value = T, invert = T) %>% regexPipes::grep(regex3, value = T) %>% regexPipes::grep(regex4, value = T, invert = T) %>% regexPipes::grep(regex5, value = T)
    # CTD_subset = CTD_subset[grep(regex, CTD_subset$Interaction, perl = T),,drop=F]
    
    if(length(interactions) < 1) return(empty_table)
    
    # Build a data frame to hold the targets and the direction the drug affects the target in.
    # We won't distinguish between mRNA and protein here. 
    # Populate the data frame with the targets.
    targets = future_lapply(interactions, FUN = extractTarget, drug = drug)
    targets = as.data.frame(do.call(rbind, targets), stringsAsFactors = F)
    if(nrow(targets) > 0) {
      colnames(targets) = c("Drug", "Target", "Effect")
      targets$Effect = as.integer(as.character(targets$Effect))
      
      # Remove duplicates
      targets = unique(targets)
    }
    
  rm(CTD_subset)
  
  return(targets)
  }
}

# Check the format of the data frames that calcNetworkScore() uses. 

# Get the vector of drugs.
# Update 08/16/2021: now including even the drugs without KEGG pathways.
GDSC_drug_list = read.csv(filename_generator(data_dir, data_source = "GDSC", data_type = "Drugs", extension = ".csv", processing_stage = "Processed", additional_info = "ids"), stringsAsFactors = F)
drugs = GDSC_drug_list %>% dplyr::filter(exclude != "Y") %>% dplyr::select(drug_name) %>% unlist() %>% unique() %>% str_to_lower()
# Replace "venotoclax" with "venetoclax."
drugs[drugs=="venotoclax"] = "venetoclax"

# Load the file. 
GDSC_drug_cgixns_CTD_filtered_filename = paste0(folder_generator(data_dir, data_source = "GDSC", data_type = "Drugs", processing_stage = "Processed"), "GDSC_CTD_chem_cgixns_1632082857803_filtered.rds")
GDSC_drug_cgixns_CTD_filtered = readRDS(GDSC_drug_cgixns_CTD_filtered_filename)

# Get the targets.
GDSC_drug_target_data_CTD_ML = future_lapply(drugs, FUN = getTargets3) # Returns a list. 
names(GDSC_drug_target_data_CTD_ML) = drugs

# Rename the drugs in the tables and get the drugs that have entries.
drugs_with_CTD_entries = c()
for(drug in names(GDSC_drug_target_data_CTD_ML)) {
  if(nrow(GDSC_drug_target_data_CTD_ML[[drug]]) > 0) {
    GDSC_drug_target_data_CTD_ML[[drug]]$Drug = drug
    drugs_with_CTD_entries = c(drugs_with_CTD_entries, drug)
  }
}
# Keep only the drugs with entries.
GDSC_drug_target_data_CTD_ML = GDSC_drug_target_data_CTD_ML[drugs_with_CTD_entries]

# 2) Combine CTD and TRRUST data.
drugs = union(names(GDSC_drug_target_data_CTD_ML), names(GDSC_drug_target_data_TRRUST))
GDSC_drug_target_data_CTD_TRRUST_ML = list() 
for(drug in drugs) {
  # Check if drug is in CTD data and/or in TRRUST data.
  drug_in_CTD = drug %in% names(GDSC_drug_target_data_CTD_ML)
  drug_in_TRRUST = drug %in% names(GDSC_drug_target_data_TRRUST)
  
  if(drug_in_CTD) drug_CTD_data = GDSC_drug_target_data_CTD_ML[[drug]]
  if(drug_in_TRRUST) drug_TRRUST_data = GDSC_drug_target_data_TRRUST[[drug]]
  
  if(drug_in_CTD & drug_in_TRRUST) {
    drug_data = rbind(drug_CTD_data, drug_TRRUST_data)
  } else if(drug_in_CTD & !drug_in_TRRUST) {
    drug_data = drug_CTD_data
  } else if(!drug_in_CTD & drug_in_TRRUST) {
    drug_data = drug_TRRUST_data
  }
  
  # Keep only the unique entries. 
  drug_data = drug_data %>% distinct()
  
  GDSC_drug_target_data_CTD_TRRUST_ML[[drug]] = drug_data
}

# 3) Get the pathways using PROGENy. 
n_top_genes = 500 # See Aim 1.
PROGENy_results = list()
for(drug in drugs) {
  # Load the data. 
  drug_data = GDSC_drug_target_data_CTD_TRRUST_ML[[drug]]
  
  # Keep only the drugs with at least 5 known transcriptional targets.
  if(nrow(drug_data) < 5) next
  
  # Consolidate duplicate rows (i.e., rows with the same target but different effects).
  # https://stackoverflow.com/a/10180178
  drug_data = drug_data %>% dplyr::select(Target, Effect) %>% ddply("Target", numcolwise(sum))
  rownames(drug_data) = drug_data$Target
  # Because of the consolidation in the previous step, we might have effect values > 1 or < -1. Set the ceiling and floor to be 1 and -1, respectively. 
  drug_data$Effect = ifelse(drug_data$Effect > 1, 1, drug_data$Effect)
  drug_data$Effect = ifelse(drug_data$Effect < -1, -1, drug_data$Effect)
  # Convert to matrix for input into PROGENy().
  drug_data = drug_data %>% dplyr::select(Effect) %>% as.matrix
  
  PROGENy_pathways = progeny(drug_data, scale = T, organism = "Human", top = n_top_genes,
    perm = 1, verbose = T)
  rownames(PROGENy_pathways) = drug
  PROGENy_results[[drug]] = PROGENy_pathways
}

drug_PROGENy_data = do.call(rbind, PROGENy_results)

# Save.
GDSC_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "GDSC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
saveRDS(drug_PROGENy_data, GDSC_drug_PROGENy_data_filename)
```

### Cell-line data
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# Code taken from Aim 1.
# But this time, we won't be converting the Z-scores to p-values. 
# Load the list of DEG genes for each patient sample if it is not loaded already. 
deg_list_filename = filename_generator(data_dir = data_dir, data_source = "NCI-60", data_type = "Expression", data_subtype = "DEG_list", extension = ".rds", processing_stage = "Processed", processing = "YuGene", gene_identifier = "HUGO", DE_criterion = "all_genes", full_path = T)
deg_list = readRDS(deg_list_filename)

# Loop over each patient sample/cell line.
PROGENy_results_cl = list() # cl = cell line.

n_top_genes = 500 
#system.time({
for(cell_line in names(deg_list)) {
  print(paste0("Processing sample ", cell_line, "."))
  
  # Load the DEG list.
  degs = deg_list[[cell_line]][["DEGs"]]
  
  # Determine the level of dysregulation of the pathways.
  degs_corrected = hadamard.prod(degs, -1) # Correction is needed before we feed degs into PROGENy() because when we calculated the rank change, 1) genes were sorted in DESCENDING order of expression, and 2) the rank change was calculated as tumor - normal. THEREFORE, a gene with a NEGATIVE rank change was UPREGULATED going from normal to tumor, and a gene with a POSITIVE rank change was DOWNREGULATED going from normal to tumor. 
  # Normalize. 
  degs_corrected = degs_corrected / length(degs)
  
  # Perform PROGENy analysis. 
  PROGENy_pathways_cl = progeny(degs_corrected, scale = T, organism = "Human", top = n_top_genes, perm = 1, verbose = T)
  rownames(PROGENy_pathways_cl) = cell_line
  PROGENy_results_cl[[cell_line]] = PROGENy_pathways_cl

}
#})

cl_PROGENy_data = do.call(rbind, PROGENy_results_cl)

# Save.
NCI60_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "GDSC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
saveRDS(cl_PROGENy_data, NCI60_cl_PROGENy_data_filename)
```

### Putting it together
```{r}
# Using the pData as a template, generate the ML inputs. 
# Load the dictionary for the standardization of NCI-60 cell-line names.
NCI60_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "GDSC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
GDSC_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "GDSC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
NCI60_cell_line_name_dict_filename = paste0(folder_generator(data_dir, data_source = "NCI-60", data_type = "Metadata", processing_stage = "Processed"), "NCI-60_cell_line_names_ALMANAC_GEO_GDSC_CCLE.csv")

cl_PROGENy_data = readRDS(NCI60_cl_PROGENy_data_filename)
drug_PROGENy_data = readRDS(GDSC_drug_PROGENy_data_filename)
NCI60_cell_line_name_dict = read.csv(NCI60_cell_line_name_dict_filename)
# Load the GDSC data.
GDSC_1_filename = paste0(folder_generator(data_dir = data_dir, data_source = "GDSC", data_type = "pData", processing_stage = "Raw"), "GDSC1 PANCANCER_IC_Sun Sep 19 20 44 07 2021.csv")
GDSC_2_filename = paste0(folder_generator(data_dir = data_dir, data_source = "GDSC", data_type = "pData", processing_stage = "Raw"), "GDSC2 PANCANCER_IC_Sun Sep 19 20 44 47 2021.csv")
GDSC1 = read.csv(GDSC_1_filename)
GDSC2 = read.csv(GDSC_2_filename)
# Standardize drug and cell-line names.
GDSC2$Drug.name = str_to_lower(GDSC2$Drug.name)
GDSC1$Drug.name = str_to_lower(GDSC1$Drug.name)
a = NCI60_cell_line_name_dict$GDSC_name
b = NCI60_cell_line_name_dict$Standardized_name
names(b) = a
GDSC1$Cell.line.name =  b[match(GDSC1$Cell.line.name, a)]
GDSC2$Cell.line.name =  b[match(GDSC2$Cell.line.name, a)]

# See how many of the NCI-60 cell lines are in the GDSC data.
GDSC1_cell_lines = GDSC1$Cell.line.name %>% unique
NCI60_cell_lines_GDSC = NCI60_cell_line_name_dict$Standardized_name
sum(GDSC1_cell_lines %in% NCI60_cell_lines_GDSC)
intersect(GDSC1_cell_lines, NCI60_cell_lines_GDSC)
setdiff(NCI60_cell_lines_GDSC, GDSC1_cell_lines)

GDSC2_cell_lines = GDSC2$Cell.line.name %>% unique
NCI60_cell_lines_GDSC = NCI60_cell_line_name_dict$Standardized_name
sum(GDSC2_cell_lines %in% NCI60_cell_lines_GDSC)
intersect(GDSC2_cell_lines, NCI60_cell_lines_GDSC)
setdiff(NCI60_cell_lines_GDSC, GDSC2_cell_lines)

# Subset the GDSC data to include only the drugs that have PROGENy data and the cell lines that are in the NCI-60.
drugs = rownames(drug_PROGENy_data)
cell_lines = rownames(cl_PROGENy_data)
GDSC1_subset_ML = GDSC1 %>% dplyr::filter(Drug.name %in% drugs & Cell.line.name %in% cell_lines)
GDSC2_subset_ML = GDSC2 %>% dplyr::filter(Drug.name %in% drugs & Cell.line.name %in% cell_lines)
setdiff(drugs, GDSC2$Drug.name %>% unique)
GDSC1_subset_ML$Drug.name %>% unique %>% length
GDSC2_subset_ML$Drug.name %>% unique %>% length
GDSC1_subset_ML_filename = filename_generator(data_dir = data_dir, data_source = "GDSC", extension = ".rds", data_set = "1", data_type = "pData", processing_stage = "Processed", additional_info = "ML")
GDSC2_subset_ML_filename = filename_generator(data_dir = data_dir, data_source = "GDSC", extension = ".rds", data_set = "2", data_type = "pData", processing_stage = "Processed", additional_info = "ML")
saveRDS(GDSC1_subset_ML, GDSC1_subset_ML_filename)
saveRDS(GDSC2_subset_ML, GDSC2_subset_ML_filename)

# For each row, get the appropriate drug-target and cell-line data and affix. 
# Assign grouping for n-fold CV.
set.seed(1026)
descriptor_mat = matrix(, nrow = nrow(GDSC1_subset_ML), ncol = (ncol(cl_PROGENy_data) + ncol(drug_PROGENy_data)))
for(i in 1:nrow(GDSC1_subset_ML)) {
  cell_line = GDSC1_subset_ML[i,"Cell.line.name"]
  drug = GDSC1_subset_ML[i,"Drug.name"]
  
  cell_line_data = cl_PROGENy_data[rownames(cl_PROGENy_data)==cell_line,]
  drug_data = drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,]
  if(length(cell_line_data) < 1) cell_line_data = rep(NA, length(drug_data)) # This is necessary because if cell_line_data is shorter than drug_data_1, when we put them together in the next step, R will fill in the missing values by repeating the vector. 
  
  descriptor_mat[i,] = c(cell_line_data, drug_data)
}
colnames(descriptor_mat) = c(paste0("cl_", colnames(cl_PROGENy_data)), paste0("drug_", colnames(drug_PROGENy_data)))
# Filter out the incomplete cases.
descriptor_mat = descriptor_mat %>% .[complete.cases(.),]

# Add metadata and target variable. 
GDSC1_subset_ML_corrected = GDSC1_subset_ML
GDSC1_subset_ML_corrected$IC50 = GDSC1_subset_ML_corrected$IC50 %>% as.character %>% as.numeric %>% hadamard.prod(-1) # Convert from factor to numeric and multiply by -1 to get -ln(IC50), since IC50 values in the raw data are given as -ln(IC50). https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources//Data/suppData/SuppExpProc_SuppRef.pdf
descriptor_df = cbind(GDSC1_subset_ML_corrected %>% dplyr::select(Drug.name, Cell.line.name, IC50), as.data.frame(descriptor_mat))
descriptor_df$Group = descriptor_df$Cell.line.name %>% as.factor %>% as.numeric
descriptor_df = descriptor_df %>% dplyr::relocate(Group, .before = Drug.name)

# Save. 
# 09/01/2021: Because of the smaller number of rows, the full dataset will also be the ML subset. This is redundant, but save descriptor_df under both names (full and subset.)
# Previous filename: /Users/alonzowolf/Dropbox/Work/Thesis_projects/Common_material/Data/Processed_data/Machine_learning/[Old_data/ as of 09/01/2021]GDSC1_PROGENy_ML_DF.csv
#descriptor_df = read.csv("/Users/alonzowolf/Dropbox/Work/Thesis_projects/Common_material/Data/Processed_data/Machine_learning/Old_data/GDSC1_PROGENy_ML_DF.csv")
full_descriptor_df_filename = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", extension = ".csv")
subset_descriptor_df_filename = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
write.csv(descriptor_df, full_descriptor_df_filename, row.names = F)
write.csv(descriptor_df, subset_descriptor_df_filename, row.names = F)
```

### Random data
```{r}
# Load the descriptor data frame.
full_descriptor_df_filename = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", extension = ".csv")
descriptor_df = read.csv(full_descriptor_df_filename, stringsAsFactors = F)

# Matrix of scrambled descriptors.
# https://stats.stackexchange.com/a/10229
set.seed(1026)
descriptor_mat_rand = descriptor_df %>% .[,5:ncol(.)] %>% as.matrix %>% .[sample.int(nrow(.)),] %>% .[,sample.int(ncol(.))]
colnames(descriptor_mat_rand) = colnames(descriptor_df)[5:ncol(descriptor_df)]
# Cbind to form data frame.
full_descriptor_rand_df = cbind(descriptor_df[,1:4], descriptor_mat_rand)
full_descriptor_rand_df_filename = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "random", extension = ".csv")
write.csv(full_descriptor_rand_df, full_descriptor_rand_df_filename , row.names = F)
```

## ALMANAC
### Drug-target data: descriptor generation
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# 1) Get the transcriptional effects.
# 1a) From TRRUST.
ALMANAC_drug_target_data_TRRUST_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "Drugs", data_set = "TRRUST", extension = ".rds", additional_info = "target_data", processing_stage = "Processed")
ALMANAC_drug_target_data_TRRUST = readRDS(ALMANAC_drug_target_data_TRRUST_filename)

# 1b) From CTD (transcriptional).
# Naming the modified function "getTargets3," how creative. 
getTargets3 = function(drug) {
  # For drug "drug," get the targets. 
  print(paste0("Getting targets for drug ", drug, "."))
  
  # Create an empty table to be returned for drugs without entries.
  empty_table = data.frame(
    Drug = character(),
    Target = character(),
    Effect = character(),
    stringsAsFactors = F
  )
  
  # Load the chemical-gene interactions from CTD (saved as a CSV file in /Data/CTD/ALMANAC.)
  possibleError = tryCatch(
    {
      # ALMANAC_drug_cgixns_CTD_filtered should already be loaded in the global environment. 
      # Keep only the interactions for the drug "drug" and found in humans.
      regex = paste0("^", drug, "$")
      CTD_subset = ALMANAC_drug_cgixns_CTD_filtered[ALMANAC_drug_cgixns_CTD_filtered$OrganismID=="9606" & (ALMANAC_drug_cgixns_CTD_filtered$ChemicalName==drug | ALMANAC_drug_cgixns_CTD_filtered$Input==drug | base::grepl(regex, ALMANAC_drug_cgixns_CTD_filtered$Input, ignore.case = T) | base::grepl(regex, ALMANAC_drug_cgixns_CTD_filtered$ChemicalName, ignore.case = T)),,drop=F]
    }, error=function(cond) {
      message(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, ":"))
      message(cond)
    }
  )
  
  if(inherits(possibleError, "error") | !exists("CTD_subset")) {
    print(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, "."))
    return(empty_table)
  }
  
  else {
    if(nrow(CTD_subset) < 1) return(empty_table)
    
    # We want only the interactions that include "[chemical name] (and not an analog) ... results in decreased/increased action/expression of [X protein/mRNA]."
    # regex = paste0(drug, " (?!\\banalog\\b)(binds to and results in |results in(\\s| (de|in)creased [^\\s]+ of and results in ))(de|in)creased (activity|expression) of") # Yes, there should be no space between "(?!\\banalog\\b)" and "results."
    # Standardize the drug name. 
    drug = CTD_subset$ChemicalName
    drug = drug[complete.cases(drug)][1]
  
    regex1 = paste0(" (?!\\banalog\\b)") # This will be set to negative, because we do NOT want the analogs. Just the drug itself. 
    regex2 = "(the reaction \\[|susceptibility to|co-treated|modified form)" # This will be set to negative, because we do NOT want the other drug-chemical interactions that the drug in question affects OR direct gene effects affected by other drugs. Just the direct gene effects. We also don't want co-treatments (for right now.) Finally, we don't want the modified forms of proteins. 
    regex3 = "results in (de|in)creased (activity|expression) of" # This will be set to positive, because we want to make sure that this phrase is present. 
    regex4 = "\\[" # We do NOT want something like "Carboplatin results in increased activity of [NFKB1 protein binds to RELA protein]"
    regex5 = "mRNA" # This will be set to positive, because we only want transcriptional effects this time. 
    # We will use the regexPipes library to do this. 
    interactions = CTD_subset$Interaction %>% regexPipes::grep(drug, fixed = T, value = T) %>% regexPipes::grep(regex1, perl = T, value = T) %>% regexPipes::grep(regex2, value = T, invert = T) %>% regexPipes::grep(regex3, value = T) %>% regexPipes::grep(regex4, value = T, invert = T) %>% regexPipes::grep(regex5, value = T)
    # CTD_subset = CTD_subset[grep(regex, CTD_subset$Interaction, perl = T),,drop=F]
    
    if(length(interactions) < 1) return(empty_table)
    
    # Build a data frame to hold the targets and the direction the drug affects the target in.
    # We won't distinguish between mRNA and protein here. 
    # Populate the data frame with the targets.
    targets = future_lapply(interactions, FUN = extractTarget, drug = drug)
    targets = as.data.frame(do.call(rbind, targets), stringsAsFactors = F)
    if(nrow(targets) > 0) {
      colnames(targets) = c("Drug", "Target", "Effect")
      targets$Effect = as.integer(as.character(targets$Effect))
      
      # Remove duplicates
      targets = unique(targets)
    }
    
  rm(CTD_subset)
  
  return(targets)
  }
}

# Check the format of the data frames that calcNetworkScore() uses. 

# Get the vector of drugs.
# Update 08/16/2021: now including even the drugs without KEGG pathways.
ALMANAC_drug_list = read.csv(filename_generator(data_dir, data_source = "ALMANAC", data_type = "Drugs", extension = ".csv", processing_stage = "Processed", additional_info = "ids"), stringsAsFactors = F)
drugs = ALMANAC_drug_list %>% dplyr::filter(exclude != "Y") %>% dplyr::select(drug_name) %>% unlist() %>% unique() %>% str_to_lower()
# Replace "venotoclax" with "venetoclax."
drugs[drugs=="venotoclax"] = "venetoclax"

# Load the file. 
ALMANAC_drug_cgixns_CTD_filtered_filename = paste0(folder_generator(data_dir, data_source = "ALMANAC", data_type = "Drugs", processing_stage = "Processed"), "ALMANAC_CTD_chem_cgixns_1631950517461_filtered.rds")
ALMANAC_drug_cgixns_CTD_filtered = readRDS(ALMANAC_drug_cgixns_CTD_filtered_filename)

# Get the targets.
ALMANAC_drug_target_data_CTD_ML = future_lapply(drugs, FUN = getTargets3) # Returns a list. 
names(ALMANAC_drug_target_data_CTD_ML) = drugs

# Rename the drugs in the tables and get the drugs that have entries.
drugs_with_CTD_entries = c()
for(drug in names(ALMANAC_drug_target_data_CTD_ML)) {
  if(nrow(ALMANAC_drug_target_data_CTD_ML[[drug]]) > 0) {
    ALMANAC_drug_target_data_CTD_ML[[drug]]$Drug = drug
    drugs_with_CTD_entries = c(drugs_with_CTD_entries, drug)
  }
}
# Keep only the drugs with entries.
ALMANAC_drug_target_data_CTD_ML = ALMANAC_drug_target_data_CTD_ML[drugs_with_CTD_entries]

# 2) Combine CTD and TRRUST data.
drugs = union(names(ALMANAC_drug_target_data_CTD_ML), names(ALMANAC_drug_target_data_TRRUST))
ALMANAC_drug_target_data_CTD_TRRUST_ML = list() 
for(drug in drugs) {
  # Check if drug is in CTD data and/or in TRRUST data.
  drug_in_CTD = drug %in% names(ALMANAC_drug_target_data_CTD_ML)
  drug_in_TRRUST = drug %in% names(ALMANAC_drug_target_data_TRRUST)
  
  if(drug_in_CTD) drug_CTD_data = ALMANAC_drug_target_data_CTD_ML[[drug]]
  if(drug_in_TRRUST) drug_TRRUST_data = ALMANAC_drug_target_data_TRRUST[[drug]]
  
  if(drug_in_CTD & drug_in_TRRUST) {
    drug_data = rbind(drug_CTD_data, drug_TRRUST_data)
  } else if(drug_in_CTD & !drug_in_TRRUST) {
    drug_data = drug_CTD_data
  } else if(!drug_in_CTD & drug_in_TRRUST) {
    drug_data = drug_TRRUST_data
  }
  
  # Keep only the unique entries. 
  drug_data = drug_data %>% distinct()
  
  ALMANAC_drug_target_data_CTD_TRRUST_ML[[drug]] = drug_data
}

# 3) Get the pathways using PROGENy. 
n_top_genes = 500 # See Aim 1.
PROGENy_results = list()
for(drug in drugs) {
  # Load the data. 
  drug_data = ALMANAC_drug_target_data_CTD_TRRUST_ML[[drug]]
  
  # Keep only the drugs with at least 5 known transcriptional targets.
  if(nrow(drug_data) < 5) next
  
  # Consolidate duplicate rows (i.e., rows with the same target but different effects).
  # https://stackoverflow.com/a/10180178
  drug_data = drug_data %>% dplyr::select(Target, Effect) %>% ddply("Target", numcolwise(sum))
  rownames(drug_data) = drug_data$Target
  # Because of the consolidation in the previous step, we might have effect values > 1 or < -1. Set the ceiling and floor to be 1 and -1, respectively. 
  drug_data$Effect = ifelse(drug_data$Effect > 1, 1, drug_data$Effect)
  drug_data$Effect = ifelse(drug_data$Effect < -1, -1, drug_data$Effect)
  # Convert to matrix for input into PROGENy().
  drug_data = drug_data %>% dplyr::select(Effect) %>% as.matrix
  
  PROGENy_pathways = progeny(drug_data, scale = T, organism = "Human", top = n_top_genes,
    perm = 1, verbose = T)
  rownames(PROGENy_pathways) = drug
  PROGENy_results[[drug]] = PROGENy_pathways
}

drug_PROGENy_data = do.call(rbind, PROGENy_results)

# Save.
ALMANAC_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
saveRDS(drug_PROGENy_data, ALMANAC_drug_PROGENy_data_filename)
rm(drug_PROGENy_data)
gc()
```

### Cell-line data
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# Code taken from Aim 1.
# But this time, we won't be converting the Z-scores to p-values. 
# Load the list of DEG genes for each patient sample if it is not loaded already. 
deg_list_filename = filename_generator(data_dir = data_dir, data_source = "NCI-60", data_type = "Expression", data_subtype = "DEG_list", extension = ".rds", processing_stage = "Processed", processing = "YuGene", gene_identifier = "HUGO", DE_criterion = "all_genes", full_path = T)
deg_list = readRDS(deg_list_filename)

# Loop over each patient sample/cell line.
PROGENy_results_cl = list() # cl = cell line.

n_top_genes = 500 
#system.time({
for(cell_line in names(deg_list)) {
  print(paste0("Processing sample ", cell_line, "."))
  
  # Load the DEG list.
  degs = deg_list[[cell_line]][["DEGs"]]
  
  # Determine the level of dysregulation of the pathways.
  degs_corrected = hadamard.prod(degs, -1) # Correction is needed before we feed degs into PROGENy() because when we calculated the rank change, 1) genes were sorted in DESCENDING order of expression, and 2) the rank change was calculated as tumor - normal. THEREFORE, a gene with a NEGATIVE rank change was UPREGULATED going from normal to tumor, and a gene with a POSITIVE rank change was DOWNREGULATED going from normal to tumor. 
  # Normalize. 
  degs_corrected = degs_corrected / length(degs)
  
  # Perform PROGENy analysis. 
  PROGENy_pathways_cl = progeny(degs_corrected, scale = T, organism = "Human", top = n_top_genes, perm = 1, verbose = T)
  rownames(PROGENy_pathways_cl) = cell_line
  PROGENy_results_cl[[cell_line]] = PROGENy_pathways_cl

}
#})

cl_PROGENy_data = do.call(rbind, PROGENy_results_cl)

# Save.
NCI60_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
saveRDS(cl_PROGENy_data, NCI60_cl_PROGENy_data_filename)
```

### Putting it together
```{r}
# Using the pData as a template, generate the ML inputs. 
# Load the dictionary for the standardization of NCI-60 cell-line names.
NCI60_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
ALMANAC_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
NCI60_cell_line_name_dict_filename = paste0(folder_generator(data_dir, data_source = "NCI-60", data_type = "Metadata", processing_stage = "Processed"), "NCI-60_cell_line_names_ALMANAC_GEO_GDSC_CCLE.csv")

cl_PROGENy_data = readRDS(NCI60_cl_PROGENy_data_filename)
drug_PROGENy_data = readRDS(ALMANAC_drug_PROGENy_data_filename)
NCI60_cell_line_name_dict = read.csv(NCI60_cell_line_name_dict_filename)
# Load the ALMANAC data.
# Combos.
pData_combos = readRDS(filename_generator(data_dir, data_source = "ALMANAC", data_type = "pData", extension = ".rds", processing_stage = "Processed"))
pData_combos$DRUGNAME1 = str_to_lower(pData_combos$DRUGNAME1)
pData_combos$DRUGNAME2 = str_to_lower(pData_combos$DRUGNAME2)
# Singles.
pData_singles = readRDS(filename_generator(data_dir, data_source = "ALMANAC", data_type = "pData", extension = ".rds", processing_stage = "Processed_singles"))
pData_singles$DRUGNAME1 = str_to_lower(pData_singles$DRUGNAME1)

# Subset the ALMANAC data to include only the drugs that have PROGENy data.
drugs = rownames(drug_PROGENy_data)
cell_lines = rownames(cl_PROGENy_data)
# Combos.
pData_combos_subset_ML = pData_combos %>% dplyr::filter(DRUGNAME1 %in% drugs & DRUGNAME2 %in% drugs & Sample %in% cell_lines)
setdiff(drugs, union(pData_combos$DRUGNAME1, pData_combos$DRUGNAME2) %>% unique)
union(pData_combos_subset_ML$DRUGNAME1, pData_combos_subset_ML$DRUGNAME2) %>% unique %>% length
# Singles.
pData_singles_subset_ML = pData_singles %>% dplyr::filter(Drugs %in% drugs & Sample %in% cell_lines)
setdiff(drugs, pData_singles$Drugs %>% unique)
union(pData_singles_subset_ML$DRUGNAME1, pData_singles_subset_ML$DRUGNAME2) %>% unique %>% length

# For each row, get the appropriate drug-target and cell-line data and affix. 
# Assign grouping for n-fold CV.
# Combos.
set.seed(1026)
descriptor_mat_1 = matrix(, nrow = nrow(pData_combos_subset_ML), ncol = (ncol(cl_PROGENy_data) + ncol(drug_PROGENy_data))) # Drug effects summed into one.
descriptor_mat_2 = matrix(, nrow = nrow(pData_combos_subset_ML), ncol = (ncol(cl_PROGENy_data) + (2*ncol(drug_PROGENy_data)))) # Each drug gets its own set of columns.
for(i in 1:nrow(pData_combos_subset_ML)) {
  print(paste0("Adding PROGENy data for row ", i, "."))
  
  cell_line = pData_combos_subset_ML[i,"Sample"] %>% unlist
  drugs = pData_combos_subset_ML[i,"Drugs"] %>% str_split("_") %>% unlist
  
  drug_data_mat = matrix(, nrow = length(drugs), ncol = ncol(drug_PROGENy_data))
  drug_data_2 = c()
  for(j in 1:length(drugs)) {
    drug = drugs[j]
    drug_data_mat[j,] = drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,]
    drug_data_2 = c(drug_data_2, drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,])
  }
  
  cell_line_data = cl_PROGENy_data[rownames(cl_PROGENy_data)==cell_line,]
  drug_data_1 = drug_data_mat %>% apply(2, sum, na.rm = T)
  if(length(cell_line_data) < 1) cell_line_data = rep(NA, length(drug_data_1))
  
  descriptor_mat_1[i,] = c(cell_line_data, drug_data_1)
  descriptor_mat_2[i,] = c(cell_line_data, drug_data_2)
}
colnames(descriptor_mat_1) = c(paste0("cl_", colnames(cl_PROGENy_data)), paste0("drug_", colnames(drug_PROGENy_data)))
colnames(descriptor_mat_2) = c(paste0("cl_", colnames(cl_PROGENy_data)), paste0("drug_1_", colnames(drug_PROGENy_data)), paste0("drug_2_", colnames(drug_PROGENy_data)))
# Filter out the incomplete cases.
descriptor_mat_1 = descriptor_mat_1 %>% .[complete.cases(.),]
descriptor_mat_2 = descriptor_mat_2 %>% .[complete.cases(.),]

# Singles.
set.seed(1026)
descriptor_mat_singles =  matrix(, nrow = nrow(pData_singles_subset_ML), ncol = (ncol(cl_PROGENy_data) + ncol(drug_PROGENy_data))) 
for(i in 1:nrow(pData_singles_subset_ML)) {
  print(paste0("Adding PROGENy data for row ", i, "."))
  
  cell_line = pData_singles_subset_ML[i,"Sample"] %>% unlist
  drug = pData_singles_subset_ML[i,"Drugs"] %>% unlist
  
  cell_line_data = cl_PROGENy_data[rownames(cl_PROGENy_data)==cell_line,]
  drug_data = drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,]
  if(length(cell_line_data) < 1) cell_line_data = rep(NA, length(drug_data))
  descriptor_mat_singles[i,] = c(cell_line_data, drug_data)
  
}
colnames(descriptor_mat_singles) = c(paste0("cl_", colnames(cl_PROGENy_data)), paste0("drug_", colnames(drug_PROGENy_data)))
# Filter out the incomplete cases.
descriptor_mat_singles = descriptor_mat_singles %>% .[complete.cases(.),]

# Add metadata and target variable. 
# Combos.
descriptor_df_1 = cbind(pData_combos_subset_ML %>% dplyr::select(Drugs, Sample, BESTPERCENTGROWTH), as.data.frame(descriptor_mat_1))
descriptor_df_2 = cbind(pData_combos_subset_ML %>% dplyr::select(Drugs, Sample, BESTPERCENTGROWTH), as.data.frame(descriptor_mat_2))
#descriptor_df_1$Group = descriptor_df_1$Sample %>% as.factor %>% as.numeric
descriptor_df_1 = descriptor_df_1 %>% dplyr::relocate(Sample, .before = Drugs)
descriptor_df_2 = descriptor_df_2 %>% dplyr::relocate(Sample, .before = Drugs)
# Singles.
descriptor_df_singles = cbind(pData_singles_subset_ML %>% dplyr::select(Drugs, Sample, BESTPERCENTGROWTH), as.data.frame(descriptor_mat_singles))
#descriptor_df_1$Group = descriptor_df_1$Sample %>% as.factor %>% as.numeric
descriptor_df_singles = descriptor_df_singles %>% dplyr::relocate(Sample, .before = Drugs)
# Combined.
descriptor_df_combined = rbind(descriptor_df_1, descriptor_df_singles)

# Save. 
ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")
write.csv(descriptor_df_1, ALMANAC_PROGENy_ml_df_1_filename, row.names = F)
write.csv(descriptor_df_2, ALMANAC_PROGENy_ml_df_2_filename, row.names = F)
write.csv(descriptor_df_singles, ALMANAC_PROGENy_ml_df_singles_filename, row.names = F)
write.csv(descriptor_df_combined, ALMANAC_PROGENy_ml_df_combined_filename, row.names = F)
```

### Spot-checking subset
```{r}
# Take a subset for spot-checking.
subset_proportion = 0.2 # A subset proportion of 20% will give us roughly 460 entries per cell line. If this is further split into training/test at an 80:20 ratio, this will give us 368 observations for the test, 92 for the training. I think that should be reasonable. Still, that's like 28000 rows. If we did 1/5 of that (4% subset), that would give us 74 observations for the test, 18 for the training. Maybe a 5% subset? 

ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")

input_filename_list = list()
input_filename_list[["S1"]] = ALMANAC_PROGENy_ml_df_1_filename
input_filename_list[["S2"]] = ALMANAC_PROGENy_ml_df_2_filename
input_filename_list[["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename
input_filename_list[["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename

output_filename_list = list()
output_filename_list[["S1"]] = ALMANAC_PROGENy_ml_df_1_filename %>% regexPipes::gsub("full", "subset")
output_filename_list[["S2"]] = ALMANAC_PROGENy_ml_df_2_filename %>% regexPipes::gsub("full", "subset")
output_filename_list[["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename %>% regexPipes::gsub("full", "subset")
output_filename_list[["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename %>% regexPipes::gsub("full", "subset")

for(df in names(input_filename_list)) {
  input_filename = input_filename_list[[df]]
  output_filename = output_filename_list[[df]]
  if(file.exists(output_filename)) {
    print(paste0("The file ", output_filename, " has already been generated. Moving on."))
    next
  }
  
  descriptor_df = read.csv(input_filename, stringsAsFactors = F)
  print(nrow(descriptor_df))
  
  # Rule of thumb: sample size >= 25 should be enough for Pearson's correlation. Similar for Spearman's?
  #https://www.researchgate.net/post/What-is-the-minimum-sample-size-to-run-Pearsons-R
  subset_proportion_final = ifelse(subset_proportion * nrow(descriptor_df) / 60 < 25, 1, subset_proportion)  
  
  # Randomly select treatments (drug combos or single drugs) for each cell line as sample.
  sample_indices = c()
    for(cell_line in unique(descriptor_df$Sample)) {
      # Get all the indices corresponding to entries of that cell line.
      indices = which(descriptor_df$Sample==cell_line)
  
      # Randomly sample from those indices. 
      subset_size = floor(length(indices) * subset_proportion_final)
      set.seed(1026)
      sample_indices = c(sample_indices, base::sample(indices, subset_size))
    }
  
  # Subset using sample_indices.
  descriptor_df_spotchecking_sample = descriptor_df[sample_indices,]
  
  # Save.
  write.csv(descriptor_df_spotchecking_sample, output_filename, row.names = F)
}

rm(descriptor_df_spotchecking_sample)
gc()
```

### Random data
```{r}
ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")

input_filename_list = list()
input_filename_list[["S1"]] = ALMANAC_PROGENy_ml_df_1_filename
input_filename_list[["S2"]] = ALMANAC_PROGENy_ml_df_2_filename
input_filename_list[["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename
input_filename_list[["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename

output_filename_list = list()
output_filename_list[["S1"]] = ALMANAC_PROGENy_ml_df_1_filename %>% regexPipes::gsub("full", "subset")
output_filename_list[["S2"]] = ALMANAC_PROGENy_ml_df_2_filename %>% regexPipes::gsub("full", "subset")
output_filename_list[["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename %>% regexPipes::gsub("full", "subset")
output_filename_list[["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename %>% regexPipes::gsub("full", "subset")

for(df in names(input_filename_list)) {
  additional_info_name = switch(df, 
                         "S1" = "S1", 
                         "S2" = "S2", 
                         "Singles" = "singles",
                         "Combined" = "combined")
  
  # Load the descriptor data frame.
  subset_descriptor_df_filename = output_filename_list[[df]]
  subset_descriptor_df = read.csv(subset_descriptor_df_filename, stringsAsFactors = F)
  
  # Matrix of scrambled descriptors.
  # https://stats.stackexchange.com/a/10229
  set.seed(1026)
  descriptor_mat_rand = subset_descriptor_df %>% .[,5:ncol(.)] %>% as.matrix %>% .[sample.int(nrow(.)),] %>% .[,sample.int(ncol(.))]
  colnames(descriptor_mat_rand) = colnames(subset_descriptor_df)[5:ncol(subset_descriptor_df)]
  # Cbind to form data frame.
  full_descriptor_rand_df = cbind(subset_descriptor_df[,1:4], descriptor_mat_rand)
  full_descriptor_rand_df_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = paste0(additional_info_name, "_random"), extension = ".csv")
  write.csv(full_descriptor_rand_df, full_descriptor_rand_df_filename , row.names = F)
}
```

## O'Neil
### Drug-target data: descriptor generation
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# 1) Get the transcriptional effects.
# 1a) From TRRUST.
ONeil_drug_target_data_TRRUST_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "Drugs", data_set = "TRRUST", extension = ".rds", additional_info = "target_data", processing_stage = "Processed")
ONeil_drug_target_data_TRRUST = readRDS(ONeil_drug_target_data_TRRUST_filename)

# 1b) From CTD (transcriptional).
# Naming the modified function "getTargets3," how creative. 
getTargets3 = function(drug) {
  # For drug "drug," get the targets. 
  print(paste0("Getting targets for drug ", drug, "."))
  
  # Create an empty table to be returned for drugs without entries.
  empty_table = data.frame(
    Drug = character(),
    Target = character(),
    Effect = character(),
    stringsAsFactors = F
  )
  
  # Load the chemical-gene interactions from CTD (saved as a CSV file in /Data/CTD/ONeil.)
  possibleError = tryCatch(
    {
      # ONeil_drug_cgixns_CTD_filtered should already be loaded in the global environment. 
      # Keep only the interactions for the drug "drug" and found in humans.
      regex = paste0("^", drug, "$")
      CTD_subset = ONeil_drug_cgixns_CTD_filtered[ONeil_drug_cgixns_CTD_filtered$OrganismID=="9606" & (ONeil_drug_cgixns_CTD_filtered$ChemicalName==drug | ONeil_drug_cgixns_CTD_filtered$Input==drug | base::grepl(regex, ONeil_drug_cgixns_CTD_filtered$Input, ignore.case = T) | base::grepl(regex, ONeil_drug_cgixns_CTD_filtered$ChemicalName, ignore.case = T)),,drop=F]
    }, error=function(cond) {
      message(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, ":"))
      message(cond)
    }
  )
  
  if(inherits(possibleError, "error") | !exists("CTD_subset")) {
    print(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, "."))
    return(empty_table)
  }
  
  else {
    if(nrow(CTD_subset) < 1) return(empty_table)
    
    # We want only the interactions that include "[chemical name] (and not an analog) ... results in decreased/increased action/expression of [X protein/mRNA]."
    # regex = paste0(drug, " (?!\\banalog\\b)(binds to and results in |results in(\\s| (de|in)creased [^\\s]+ of and results in ))(de|in)creased (activity|expression) of") # Yes, there should be no space between "(?!\\banalog\\b)" and "results."
    # Standardize the drug name. 
    drug = CTD_subset$ChemicalName
    drug = drug[complete.cases(drug)][1]
  
    regex1 = paste0(" (?!\\banalog\\b)") # This will be set to negative, because we do NOT want the analogs. Just the drug itself. 
    regex2 = "(the reaction \\[|susceptibility to|co-treated|modified form)" # This will be set to negative, because we do NOT want the other drug-chemical interactions that the drug in question affects OR direct gene effects affected by other drugs. Just the direct gene effects. We also don't want co-treatments (for right now.) Finally, we don't want the modified forms of proteins. 
    regex3 = "results in (de|in)creased (activity|expression) of" # This will be set to positive, because we want to make sure that this phrase is present. 
    regex4 = "\\[" # We do NOT want something like "Carboplatin results in increased activity of [NFKB1 protein binds to RELA protein]"
    regex5 = "mRNA" # This will be set to positive, because we only want transcriptional effects this time. 
    # We will use the regexPipes library to do this. 
    interactions = CTD_subset$Interaction %>% regexPipes::grep(drug, fixed = T, value = T) %>% regexPipes::grep(regex1, perl = T, value = T) %>% regexPipes::grep(regex2, value = T, invert = T) %>% regexPipes::grep(regex3, value = T) %>% regexPipes::grep(regex4, value = T, invert = T) %>% regexPipes::grep(regex5, value = T)
    # CTD_subset = CTD_subset[grep(regex, CTD_subset$Interaction, perl = T),,drop=F]
    
    if(length(interactions) < 1) return(empty_table)
    
    # Build a data frame to hold the targets and the direction the drug affects the target in.
    # We won't distinguish between mRNA and protein here. 
    # Populate the data frame with the targets.
    targets = future_lapply(interactions, FUN = extractTarget, drug = drug)
    targets = as.data.frame(do.call(rbind, targets), stringsAsFactors = F)
    if(nrow(targets) > 0) {
      colnames(targets) = c("Drug", "Target", "Effect")
      targets$Effect = as.integer(as.character(targets$Effect))
      
      # Remove duplicates
      targets = unique(targets)
    }
    
  rm(CTD_subset)
  
  return(targets)
  }
}

# Check the format of the data frames that calcNetworkScore() uses. 

# Get the vector of drugs.
# Update 08/16/2021: now including even the drugs without KEGG pathways.
ONeil_drug_list = read.csv(filename_generator(data_dir, data_source = "ONeil", data_type = "Drugs", extension = ".csv", processing_stage = "Processed", additional_info = "ids"), stringsAsFactors = F)
drugs = ONeil_drug_list %>% dplyr::filter(exclude != "Y") %>% dplyr::select(drug_name) %>% unlist() %>% unique() %>% str_to_lower()
# Replace "venotoclax" with "venetoclax."
drugs[drugs=="venotoclax"] = "venetoclax"

# Load the file. 
ONeil_drug_cgixns_CTD_filtered_filename = paste0(folder_generator(data_dir, data_source = "ONeil", data_type = "Drugs", processing_stage = "Processed"), "ONeil_CTD_chem_cgixns_1635832327332_filtered.rds")
ONeil_drug_cgixns_CTD_filtered = readRDS(ONeil_drug_cgixns_CTD_filtered_filename)

# Get the targets.
ONeil_drug_target_data_CTD_ML = future_lapply(drugs, FUN = getTargets3) # Returns a list. 
names(ONeil_drug_target_data_CTD_ML) = drugs

# Rename the drugs in the tables and get the drugs that have entries.
drugs_with_CTD_entries = c()
for(drug in names(ONeil_drug_target_data_CTD_ML)) {
  if(nrow(ONeil_drug_target_data_CTD_ML[[drug]]) > 0) {
    ONeil_drug_target_data_CTD_ML[[drug]]$Drug = drug
    drugs_with_CTD_entries = c(drugs_with_CTD_entries, drug)
  }
}
# Keep only the drugs with entries.
ONeil_drug_target_data_CTD_ML = ONeil_drug_target_data_CTD_ML[drugs_with_CTD_entries]

# 2) Combine CTD and TRRUST data.
drugs = union(names(ONeil_drug_target_data_CTD_ML), names(ONeil_drug_target_data_TRRUST))
ONeil_drug_target_data_CTD_TRRUST_ML = list() 
for(drug in drugs) {
  # Check if drug is in CTD data and/or in TRRUST data.
  drug_in_CTD = drug %in% names(ONeil_drug_target_data_CTD_ML)
  drug_in_TRRUST = drug %in% names(ONeil_drug_target_data_TRRUST)
  
  if(drug_in_CTD) drug_CTD_data = ONeil_drug_target_data_CTD_ML[[drug]]
  if(drug_in_TRRUST) drug_TRRUST_data = ONeil_drug_target_data_TRRUST[[drug]]
  
  if(drug_in_CTD & drug_in_TRRUST) {
    drug_data = rbind(drug_CTD_data, drug_TRRUST_data)
  } else if(drug_in_CTD & !drug_in_TRRUST) {
    drug_data = drug_CTD_data
  } else if(!drug_in_CTD & drug_in_TRRUST) {
    drug_data = drug_TRRUST_data
  }
  
  # Keep only the unique entries. 
  drug_data = drug_data %>% distinct()
  
  ONeil_drug_target_data_CTD_TRRUST_ML[[drug]] = drug_data
}

# 3) Get the pathways using PROGENy. 
n_top_genes = 500 # See Aim 1.
PROGENy_results = list()
for(drug in drugs) {
  # Load the data. 
  drug_data = ONeil_drug_target_data_CTD_TRRUST_ML[[drug]]
  
  # Keep only the drugs with at least 5 known transcriptional targets.
  if(nrow(drug_data) < 5) next
  
  # Consolidate duplicate rows (i.e., rows with the same target but different effects).
  # https://stackoverflow.com/a/10180178
  drug_data = drug_data %>% dplyr::select(Target, Effect) %>% ddply("Target", numcolwise(sum))
  rownames(drug_data) = drug_data$Target
  # Because of the consolidation in the previous step, we might have effect values > 1 or < -1. Set the ceiling and floor to be 1 and -1, respectively. 
  drug_data$Effect = ifelse(drug_data$Effect > 1, 1, drug_data$Effect)
  drug_data$Effect = ifelse(drug_data$Effect < -1, -1, drug_data$Effect)
  # Convert to matrix for input into PROGENy().
  drug_data = drug_data %>% dplyr::select(Effect) %>% as.matrix
  
  PROGENy_pathways = progeny(drug_data, scale = T, organism = "Human", top = n_top_genes,
    perm = 1, verbose = T)
  rownames(PROGENy_pathways) = drug
  PROGENy_results[[drug]] = PROGENy_pathways
}

drug_PROGENy_data = do.call(rbind, PROGENy_results)

# Save.
ONeil_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
saveRDS(drug_PROGENy_data, ONeil_drug_PROGENy_data_filename)
rm(drug_PROGENy_data)
gc()

# Update 2022-05-25.
# Add lapatinib.
#ONeil_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
#ONeil_drug_PROGENy_data = readRDS(ONeil_drug_PROGENy_data_filename)
#ALMANAC_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
#ALMANAC_drug_PROGENy_data = readRDS(ALMANAC_drug_PROGENy_data_filename)

#ONeil_drug_PROGENy_data = rbind(ONeil_drug_PROGENy_data, ALMANAC_drug_PROGENy_data %>% .[rownames(.)=="lapatinib",])
#rownames(ONeil_drug_PROGENy_data)[nrow(ONeil_drug_PROGENy_data)] = "lapatinib"
#saveRDS(ONeil_drug_PROGENy_data, ONeil_drug_PROGENy_data_filename)

```

### Cell-line data
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# Code taken from Aim 1.
# But this time, we won't be converting the Z-scores to p-values. 
# Load the list of DEG genes for each patient sample if it is not loaded already. 
deg_list_filename = filename_generator(data_dir = data_dir, data_source = "ONeil", data_type = "Expression", data_subtype = "DEG_list", extension = ".rds", processing_stage = "Processed", processing = "YuGene", gene_identifier = "HUGO", DE_criterion = "all_genes", full_path = T)
deg_list = readRDS(deg_list_filename)

# Loop over each patient sample/cell line.
PROGENy_results_cl = list() # cl = cell line.

n_top_genes = 500 
#system.time({
for(cell_line in names(deg_list)) {
  print(paste0("Processing sample ", cell_line, "."))
  
  # Load the DEG list.
  degs = deg_list[[cell_line]][["DEGs"]]
  
  # Determine the level of dysregulation of the pathways.
  degs_corrected = hadamard.prod(degs, -1) # Correction is needed before we feed degs into PROGENy() because when we calculated the rank change, 1) genes were sorted in DESCENDING order of expression, and 2) the rank change was calculated as tumor - normal. THEREFORE, a gene with a NEGATIVE rank change was UPREGULATED going from normal to tumor, and a gene with a POSITIVE rank change was DOWNREGULATED going from normal to tumor. 
  # Normalize. 
  degs_corrected = degs_corrected / length(degs)
  
  # Perform PROGENy analysis. 
  PROGENy_pathways_cl = progeny(degs_corrected, scale = T, organism = "Human", top = n_top_genes, perm = 1, verbose = T)
  rownames(PROGENy_pathways_cl) = cell_line
  PROGENy_results_cl[[cell_line]] = PROGENy_pathways_cl

}
#})

cl_PROGENy_data = do.call(rbind, PROGENy_results_cl)

# Save.
ONeil_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
saveRDS(cl_PROGENy_data, ONeil_cl_PROGENy_data_filename)
```

### Putting it together
```{r}
# Using the pData as a template, generate the ML inputs. 
# Load the dictionary for the standardization of ONeil cell-line names.
ONeil_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
ONeil_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
ONeil_cell_line_name_dict_filename = paste0(folder_generator(
  data_dir = data_dir,
  data_source = "ONeil",
  data_type = "Metadata",
  processing_stage = "Processed"
), "ONeil_cell_line_names_dictionary.csv")

cl_PROGENy_data = readRDS(ONeil_cl_PROGENy_data_filename)
drug_PROGENy_data = readRDS(ONeil_drug_PROGENy_data_filename)
ONeil_cell_line_name_dict = read.csv(ONeil_cell_line_name_dict_filename)
# Load the ONeil data.
# Combos.
pData_combos = readRDS(filename_generator(data_dir, data_source = "ONeil", data_type = "pData", extension = ".rds", processing_stage = "Processed"))
pData_combos$drugA_name = str_to_lower(pData_combos$drugA_name)
pData_combos$drugB_name = str_to_lower(pData_combos$drugB_name)

# Subset the ONeil data to include only the drugs and cell lines that have PROGENy data.
drugs = rownames(drug_PROGENy_data)
cell_lines = rownames(cl_PROGENy_data)
# Combos.
pData_combos_subset_ML = pData_combos %>% dplyr::filter(drugA_name %in% drugs & drugB_name %in% drugs & cell_line %in% cell_lines)
setdiff(drugs, union(pData_combos$drugA_name, pData_combos$drugB_name) %>% unique)
union(pData_combos_subset_ML$drugA_name, pData_combos_subset_ML$drugB_name) %>% unique %>% length

# For each row, get the appropriate drug-target and cell-line data and affix. 
# Assign grouping for n-fold CV.
# Combos.
set.seed(1026)
descriptor_mat_1 = matrix(, nrow = nrow(pData_combos_subset_ML), ncol = (ncol(cl_PROGENy_data) + ncol(drug_PROGENy_data))) # Drug effects summed into one.
for(i in 1:nrow(pData_combos_subset_ML)) {
  print(paste0("Adding PROGENy data for row ", i, "."))
  
  cell_line = pData_combos_subset_ML[i,"cell_line"] %>% unlist
  drugs = pData_combos_subset_ML[i,"combination_name"] %>% str_split("_") %>% unlist
  
  drug_data_mat = matrix(, nrow = length(drugs), ncol = ncol(drug_PROGENy_data))
  drug_data_2 = c()
  for(j in 1:length(drugs)) {
    drug = drugs[j]
    drug_data_mat[j,] = drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,]
    drug_data_2 = c(drug_data_2, drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,])
  }

  drug_data_1 = drug_data_mat %>% apply(2, sum, na.rm = T)
  cell_line_data = cl_PROGENy_data[rownames(cl_PROGENy_data)==cell_line,]
  if(length(cell_line_data) < 1) cell_line_data = rep(NA, length(drug_data_1)) # This is necessary because if cell_line_data is shorter than drug_data_1, when we put them together in the next step, R will fill in the missing values by repeating the vector. 
  
  descriptor_mat_1[i,] = c(cell_line_data, drug_data_1)
}
colnames(descriptor_mat_1) = c(paste0("cl_", colnames(cl_PROGENy_data)), paste0("drug_", colnames(drug_PROGENy_data)))
# Filter out the incomplete cases.
descriptor_mat_1 = descriptor_mat_1 %>% .[complete.cases(.),]

# Add metadata and target variable. 
# Combos.
descriptor_df_1 = cbind(pData_combos_subset_ML %>% dplyr::select(combination_name, cell_line, viability_mean_best), as.data.frame(descriptor_mat_1))
#descriptor_df_1$Group = descriptor_df_1$cell_line %>% as.factor %>% as.numeric
descriptor_df_1 = descriptor_df_1 %>% dplyr::relocate(cell_line, .before = combination_name)

# Save. 
ONeil_PROGENy_ml_df_combos_filename = filename_generator(data_dir = data_dir, data_source = "ONeil", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "Combos", extension = ".csv")
write.csv(descriptor_df_1, ONeil_PROGENy_ml_df_combos_filename, row.names = F)
```

## TCGA
### Drug-target data: descriptor generation
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# 1) Get the transcriptional effects.
# 1a) From TRRUST.
TCGA_drug_target_data_TRRUST_filename = filename_generator(data_dir, data_source = "TCGA", data_type = "Drugs", data_set = "TRRUST", extension = ".rds", additional_info = "target_data", processing_stage = "Processed")
TCGA_drug_target_data_TRRUST = readRDS(TCGA_drug_target_data_TRRUST_filename)

# 1b) From CTD (transcriptional).
# Naming the modified function "getTargets3," how creative. 
getTargets3 = function(drug) {
  # For drug "drug," get the targets. 
  print(paste0("Getting targets for drug ", drug, "."))
  
  # Create an empty table to be returned for drugs without entries.
  empty_table = data.frame(
    Drug = character(),
    Target = character(),
    Effect = character(),
    stringsAsFactors = F
  )
  
  # Load the chemical-gene interactions from CTD (saved as a CSV file in /Data/CTD/TCGA.)
  possibleError = tryCatch(
    {
      # TCGA_drug_cgixns_CTD_filtered should already be loaded in the global environment. 
      # Keep only the interactions for the drug "drug" and found in humans.
      regex = paste0("^", drug, "$")
      CTD_subset = TCGA_drug_cgixns_CTD_filtered[TCGA_drug_cgixns_CTD_filtered$OrganismID=="9606" & (TCGA_drug_cgixns_CTD_filtered$ChemicalName==drug | TCGA_drug_cgixns_CTD_filtered$Input==drug | base::grepl(regex, TCGA_drug_cgixns_CTD_filtered$Input, ignore.case = T) | base::grepl(regex, TCGA_drug_cgixns_CTD_filtered$ChemicalName, ignore.case = T)),,drop=F]
    }, error=function(cond) {
      message(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, ":"))
      message(cond)
    }
  )
  
  if(inherits(possibleError, "error") | !exists("CTD_subset")) {
    print(paste0("For getTargets(), there was a problem retrieving the chemical-gene interactions for the drug ", drug, "."))
    return(empty_table)
  }
  
  else {
    if(nrow(CTD_subset) < 1) return(empty_table)
    
    # We want only the interactions that include "[chemical name] (and not an analog) ... results in decreased/increased action/expression of [X protein/mRNA]."
    # regex = paste0(drug, " (?!\\banalog\\b)(binds to and results in |results in(\\s| (de|in)creased [^\\s]+ of and results in ))(de|in)creased (activity|expression) of") # Yes, there should be no space between "(?!\\banalog\\b)" and "results."
    # Standardize the drug name. 
    drug = CTD_subset$ChemicalName
    drug = drug[complete.cases(drug)][1]
  
    regex1 = paste0(" (?!\\banalog\\b)") # This will be set to negative, because we do NOT want the analogs. Just the drug itself. 
    regex2 = "(the reaction \\[|susceptibility to|co-treated|modified form)" # This will be set to negative, because we do NOT want the other drug-chemical interactions that the drug in question affects OR direct gene effects affected by other drugs. Just the direct gene effects. We also don't want co-treatments (for right now.) Finally, we don't want the modified forms of proteins. 
    regex3 = "results in (de|in)creased (activity|expression) of" # This will be set to positive, because we want to make sure that this phrase is present. 
    regex4 = "\\[" # We do NOT want something like "Carboplatin results in increased activity of [NFKB1 protein binds to RELA protein]"
    regex5 = "mRNA" # This will be set to positive, because we only want transcriptional effects this time. 
    # We will use the regexPipes library to do this. 
    interactions = CTD_subset$Interaction %>% regexPipes::grep(drug, fixed = T, value = T) %>% regexPipes::grep(regex1, perl = T, value = T) %>% regexPipes::grep(regex2, value = T, invert = T) %>% regexPipes::grep(regex3, value = T) %>% regexPipes::grep(regex4, value = T, invert = T) %>% regexPipes::grep(regex5, value = T)
    # CTD_subset = CTD_subset[grep(regex, CTD_subset$Interaction, perl = T),,drop=F]
    
    if(length(interactions) < 1) return(empty_table)
    
    # Build a data frame to hold the targets and the direction the drug affects the target in.
    # We won't distinguish between mRNA and protein here. 
    # Populate the data frame with the targets.
    targets = future_lapply(interactions, FUN = extractTarget, drug = drug)
    targets = as.data.frame(do.call(rbind, targets), stringsAsFactors = F)
    if(nrow(targets) > 0) {
      colnames(targets) = c("Drug", "Target", "Effect")
      targets$Effect = as.integer(as.character(targets$Effect))
      
      # Remove duplicates
      targets = unique(targets)
    }
    
  rm(CTD_subset)
  
  return(targets)
  }
}

# Check the format of the data frames that calcNetworkScore() uses. 

# Get the vector of drugs.
# Update 08/16/2021: now including even the drugs without KEGG pathways.
TCGA_drug_list = read.csv(filename_generator(data_dir, data_source = "TCGA", data_type = "Drugs", extension = ".csv", processing_stage = "Processed", additional_info = "ids"), stringsAsFactors = F)
drugs = TCGA_drug_list %>% dplyr::filter(exclude != "Y") %>% dplyr::select(drug_name) %>% unlist() %>% unique() %>% str_to_lower()
# Replace "venotoclax" with "venetoclax."
drugs[drugs=="venotoclax"] = "venetoclax"

# Load the file. 
TCGA_drug_cgixns_CTD_filtered_filename = paste0(folder_generator(data_dir, data_source = "TCGA", data_type = "Drugs", processing_stage = "Processed"), "TCGA_CTD_chem_cgixns_1634090448182_filtered.rds")
TCGA_drug_cgixns_CTD_filtered = readRDS(TCGA_drug_cgixns_CTD_filtered_filename)

# Get the targets.
TCGA_drug_target_data_CTD_ML = future_lapply(drugs, FUN = getTargets3) # Returns a list. 
names(TCGA_drug_target_data_CTD_ML) = drugs

# Rename the drugs in the tables and get the drugs that have entries.
drugs_with_CTD_entries = c()
for(drug in names(TCGA_drug_target_data_CTD_ML)) {
  if(nrow(TCGA_drug_target_data_CTD_ML[[drug]]) > 0) {
    TCGA_drug_target_data_CTD_ML[[drug]]$Drug = drug
    drugs_with_CTD_entries = c(drugs_with_CTD_entries, drug)
  }
}
# Keep only the drugs with entries.
TCGA_drug_target_data_CTD_ML = TCGA_drug_target_data_CTD_ML[drugs_with_CTD_entries]

# 2) Combine CTD and TRRUST data.
drugs = union(names(TCGA_drug_target_data_CTD_ML), names(TCGA_drug_target_data_TRRUST))
TCGA_drug_target_data_CTD_TRRUST_ML = list() 
for(drug in drugs) {
  # Check if drug is in CTD data and/or in TRRUST data.
  drug_in_CTD = drug %in% names(TCGA_drug_target_data_CTD_ML)
  drug_in_TRRUST = drug %in% names(TCGA_drug_target_data_TRRUST)
  
  if(drug_in_CTD) drug_CTD_data = TCGA_drug_target_data_CTD_ML[[drug]]
  if(drug_in_TRRUST) drug_TRRUST_data = TCGA_drug_target_data_TRRUST[[drug]]
  
  if(drug_in_CTD & drug_in_TRRUST) {
    drug_data = rbind(drug_CTD_data, drug_TRRUST_data)
  } else if(drug_in_CTD & !drug_in_TRRUST) {
    drug_data = drug_CTD_data
  } else if(!drug_in_CTD & drug_in_TRRUST) {
    drug_data = drug_TRRUST_data
  }
  
  # Keep only the unique entries. 
  drug_data = drug_data %>% distinct()
  
  TCGA_drug_target_data_CTD_TRRUST_ML[[drug]] = drug_data
}

# 3) Get the pathways using PROGENy. 
n_top_genes = 500 # See Aim 1.
PROGENy_results = list()
for(drug in drugs) {
  # Load the data. 
  drug_data = TCGA_drug_target_data_CTD_TRRUST_ML[[drug]]
  
  # Keep only the drugs with at least 5 known transcriptional targets.
  if(nrow(drug_data) < 5) next
  
  # Consolidate duplicate rows (i.e., rows with the same target but different effects).
  # https://stackoverflow.com/a/10180178
  drug_data = drug_data %>% dplyr::select(Target, Effect) %>% ddply("Target", numcolwise(sum))
  rownames(drug_data) = drug_data$Target
  # Because of the consolidation in the previous step, we might have effect values > 1 or < -1. Set the ceiling and floor to be 1 and -1, respectively. 
  drug_data$Effect = ifelse(drug_data$Effect > 1, 1, drug_data$Effect)
  drug_data$Effect = ifelse(drug_data$Effect < -1, -1, drug_data$Effect)
  # Convert to matrix for input into PROGENy().
  drug_data = drug_data %>% dplyr::select(Effect) %>% as.matrix
  
  PROGENy_pathways = progeny(drug_data, scale = T, organism = "Human", top = n_top_genes,
    perm = 1, verbose = T)
  rownames(PROGENy_pathways) = drug
  PROGENy_results[[drug]] = PROGENy_pathways
}

drug_PROGENy_data = do.call(rbind, PROGENy_results)

# Save.
TCGA_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "TCGA", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
saveRDS(drug_PROGENy_data, TCGA_drug_PROGENy_data_filename)
rm(drug_PROGENy_data)
gc()
```

### Patient-sample data
```{r}
#######################################
## Switch to R 4.0.2 before running! ##
#######################################

# Code taken from Aim 1.
# But this time, we won't be converting the Z-scores to p-values. 
# Load the list of DEG genes for each patient sample if it is not loaded already. 
deg_list_filename = filename_generator(data_dir = data_dir, data_source = "TCGA", data_type = "Expression", data_subtype = "DEG_list", extension = ".rds", processing_stage = "Processed", processing = "YuGene", gene_identifier = "HUGO", DE_criterion = "all_genes", full_path = T)
deg_list = readRDS(deg_list_filename)

# Loop over each patient sample/cell line.
PROGENy_results_ps = list() # cl = cell line.

n_top_genes = 500 
#system.time({
for(cell_line in names(deg_list)) {
  print(paste0("Processing sample ", cell_line, "."))
  
  # Load the DEG list.
  degs = deg_list[[cell_line]][["DEGs"]]
  
  # Determine the level of dysregulation of the pathways.
  degs_corrected = hadamard.prod(degs, -1) # Correction is needed before we feed degs into PROGENy() because when we calculated the rank change, 1) genes were sorted in DESCENDING order of expression, and 2) the rank change was calculated as tumor - normal. THEREFORE, a gene with a NEGATIVE rank change was UPREGULATED going from normal to tumor, and a gene with a POSITIVE rank change was DOWNREGULATED going from normal to tumor. 
  # Normalize. 
  degs_corrected = degs_corrected / length(degs)
  
  # Perform PROGENy analysis. 
  PROGENy_pathways_ps = progeny(degs_corrected, scale = T, organism = "Human", top = n_top_genes, perm = 1, verbose = T)
  rownames(PROGENy_pathways_ps) = cell_line
  PROGENy_results_ps[[cell_line]] = PROGENy_pathways_ps

}
#})

ps_PROGENy_data = do.call(rbind, PROGENy_results_ps)

# Save.
TCGA_ps_PROGENy_data_filename = filename_generator(data_dir, data_source = "TCGA", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "ps_data", processing_stage = "Processed")
saveRDS(ps_PROGENy_data, TCGA_ps_PROGENy_data_filename)
```

### Putting it together
```{r}
# Using the pData as a template, generate the ML inputs. 
# Load the TCGA data.
TCGA_ps_PROGENy_data_filename = filename_generator(data_dir, data_source = "TCGA", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "ps_data", processing_stage = "Processed")
TCGA_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "TCGA", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")

ps_PROGENy_data = readRDS(TCGA_ps_PROGENy_data_filename)
drug_PROGENy_data = readRDS(TCGA_drug_PROGENy_data_filename)

# Combos.
pData_combos = readRDS(filename_generator(data_dir, data_source = "TCGA", data_type = "pData", extension = ".rds", processing_stage = "Processed"))

# Subset the TCGA data to include only the drugs and cell lines that have PROGENy data.
# Edit 2022-07-11: initially, we kept only the CRs/PRs, but now for the case studies, we'll keep everything. 
drugs = rownames(drug_PROGENy_data)
samples = rownames(ps_PROGENy_data)
keep_rows = c()
for(i in 1:nrow(pData_combos)) {
  drugs_i = pData_combos[i,"Drugs"] %>% str_split("_") %>% unlist %>% as.character
  if(length(setdiff(drugs_i, drugs)) < 1) keep_rows = c(keep_rows, i)
}
pData_combos_subset_ML = pData_combos %>% .[keep_rows,] %>% dplyr::filter(Sample %in% samples) %>% .[grep(.$Drugs, "_"),] # The last part filters out single-drug treatments. Initially included this condition in dplyr::filter: & Response %in% c("Complete Response", "Partial Response")

# Generate n random drug combinations for each entry in pData_combos_subset_ML.
n_random_drug_combos = 99
drug_combos = c()
for(i in 1:nrow(pData_combos_subset_ML)) {
  drug_combos = c(drug_combos, generateRandCombos(i, pData = pData_combos_subset_ML, drug_col_name = "Drugs", n_rand = n_random_drug_combos, unique_drugs = drugs))
}

# Create a matrix with the metadata (sample name + drug combination.)
descriptor_mat_meta = cbind(rep(pData_combos_subset_ML$Sample %>% as.character, each = (n_random_drug_combos + 1)), drug_combos) %>% as.data.frame
colnames(descriptor_mat_meta) = c("Sample", "Drugs")

# For each row, get the appropriate drug-target and cell-line data and affix. 
# Assign grouping for n-fold CV.
set.seed(1026)
descriptor_mat_1 = matrix(, nrow = nrow(descriptor_mat_meta), ncol = (ncol(ps_PROGENy_data) + ncol(drug_PROGENy_data))) # Drug effects summed into one.
for(i in 1:nrow(descriptor_mat_meta)) {
  print(paste0("Adding PROGENy data for row ", i, "."))
  
  sample = descriptor_mat_meta[i,"Sample"] %>% unlist
  drugs = descriptor_mat_meta[i,"Drugs"] %>% str_split("_") %>% unlist
  
  drug_data_mat = matrix(, nrow = length(drugs), ncol = ncol(drug_PROGENy_data))
  drug_data_2 = c()
  for(j in 1:length(drugs)) {
    drug = drugs[j]
    drug_data_mat[j,] = drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,]
    drug_data_2 = c(drug_data_2, drug_PROGENy_data[rownames(drug_PROGENy_data)==drug,])
  }

  drug_data_1 = drug_data_mat %>% apply(2, sum, na.rm = T)
  sample_data = ps_PROGENy_data[rownames(ps_PROGENy_data)==sample,]
  if(length(sample_data) < 1) sample_data = rep(NA, length(drug_data_1)) # This is necessary because if sample_data is shorter than drug_data_1, when we put them together in the next step, R will fill in the missing values by repeating the vector. 
  
  descriptor_mat_1[i,] = c(sample_data, drug_data_1)
}
colnames(descriptor_mat_1) = c(paste0("ps_", colnames(ps_PROGENy_data)), paste0("drug_", colnames(drug_PROGENy_data)))
# Filter out the incomplete cases.
descriptor_mat_1 = descriptor_mat_1 %>% .[complete.cases(.),]

# Add metadata and target variable. 
descriptor_df_2 = cbind(descriptor_mat_meta, as.data.frame(descriptor_mat_1))

# Save. 
TCGA_PROGENy_ml_df_combos_filename = filename_generator(data_dir = data_dir, data_source = "TCGA", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "Combos", extension = ".csv")
write.csv(descriptor_df_2, TCGA_PROGENy_ml_df_combos_filename, row.names = F)
```

## GEO GSE58984
### Drug-target data
```{r}

```

### Patient-sample data
```{r}

```

### Putting it together
```{r}

```

## DrugComb
### Putting it together
```{r}
# Using the pData as a template, generate the ML inputs. 

# Load the dictionary for the standardization of cell-line names.
# NCI-60/ALMANAC.
NCI60_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
ALMANAC_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ALMANAC", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
NCI60_cell_line_name_dict_filename = paste0(folder_generator(data_dir, data_source = "NCI-60", data_type = "Metadata", processing_stage = "Processed"), "NCI-60_cell_line_names_ALMANAC_GEO_GDSC_CCLE_DrugComb.csv")
# O'Neil.
ONeil_cl_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "cl_data", processing_stage = "Processed")
ONeil_drug_PROGENy_data_filename = filename_generator(data_dir, data_source = "ONeil", data_type = "ML", data_set = "PROGENy", extension = ".rds", additional_info = "drug_data", processing_stage = "Processed")
ONeil_cell_line_name_dict_filename = paste0(folder_generator(
  data_dir = data_dir,
  data_source = "ONeil",
  data_type = "Metadata",
  processing_stage = "Processed"
), "ONeil_cell_line_names_dictionary.csv")
DrugComb_drug_ids_processed_filename = paste0(data_dir, "DrugComb/DrugComb_drug_ids_processed.csv")

cl_PROGENy_data = list()
drug_PROGENy_data = list()
cell_line_name_dict = list()
cl_PROGENy_data[["ALMANAC"]] = readRDS(NCI60_cl_PROGENy_data_filename)
drug_PROGENy_data[["ALMANAC"]] = readRDS(ALMANAC_drug_PROGENy_data_filename)
cl_PROGENy_data[["ONeil"]] = readRDS(ONeil_cl_PROGENy_data_filename)
drug_PROGENy_data[["ONeil"]] = readRDS(ONeil_drug_PROGENy_data_filename)
cell_line_name_dict[["ALMANAC"]] = read.csv(NCI60_cell_line_name_dict_filename)
cell_line_name_dict[["ONeil"]] = read.csv(ONeil_cell_line_name_dict_filename)
DrugComb_drug_ids_processed = read.csv(DrugComb_drug_ids_processed_filename)

# Load the DrugComb pData.
DrugComb_pData_sub_filename = paste0(data_dir, "DrugComb/pData/Processed/DrugComb_pData_sub.rds")
pData = readRDS(DrugComb_pData_sub_filename) # We already filtered out the rows with drugs/cell lines we don't have PROGENy data for in data_processing.Rmd.
# Split into ONeil and ALMANAC. 
pData_list = list()
pData_list[["ONeil"]] = pData %>% dplyr::filter(study_name == "ONEIL")
pData_list[["ALMANAC"]] = pData %>% dplyr::filter(study_name == "ALMANAC")
# Replace all the current drug names with the standardized ones. 
for(dataset in names(pData_list)) {
  drug_ids = DrugComb_drug_ids_processed %>% dplyr::filter(study_name == str_to_upper(dataset)) %>% .[complete.cases(.),]
  # Replace a with b.
  a = drug_ids$drug_row
  b = drug_ids$standardized_name
  names(b) = a
  pData_list[[dataset]]$drug_row = b[match(pData_list[[dataset]]$drug_row, a)] #str_replace_all(pData$Response, b)
  pData_list[[dataset]]$drug_col = b[match(pData_list[[dataset]]$drug_col, a)]
  
  pData_list[[dataset]]$drugs = paste(pData_list[[dataset]]$drug_row, pData_list[[dataset]]$drug_col, sep = "_")
}
# Replace all the current cell line names with the standardized ones.
for(dataset in names(pData_list)) {
  cl_name_dict = cell_line_name_dict[[dataset]]
  
  # Replace a with b.
  a = cl_name_dict$DrugComb_name
  b = cl_name_dict$Standardized_name
  names(b) = a
  pData_list[[dataset]]$cell_line_name = b[match(pData_list[[dataset]]$cell_line_name, a)]
  
}

# For each row, get the appropriate drug-target and cell-line data and affix. 
# Assign grouping for n-fold CV.
# Combos.
set.seed(1026)

descriptor_mats = list()
rows_with_missing_drug_data = list()
for(dataset in c("ONeil", "ALMANAC")) {
  descriptor_mat_1 = matrix(, 
                            nrow = nrow(pData_list[[dataset]]), 
                            ncol = (ncol(cl_PROGENy_data[[dataset]]) + ncol(drug_PROGENy_data[[dataset]])),
                            dimnames = list(paste0("row", 1:nrow(pData_list[[dataset]])), 
                               paste0("col", 1:(ncol(drug_PROGENy_data[[dataset]]) + ncol(cl_PROGENy_data[[dataset]]))))) # Drug effects summed into one.
  descriptor_mat_2 = matrix(, 
                            nrow = nrow(pData_list[[dataset]]), 
                            ncol = (ncol(cl_PROGENy_data[[dataset]]) + (2*ncol(drug_PROGENy_data[[dataset]]))),
                            dimnames = list(paste0("row", 1:nrow(pData_list[[dataset]])),
                               paste0("col", 1:(2*ncol(drug_PROGENy_data[[dataset]]) + ncol(cl_PROGENy_data[[dataset]]))))) # Each drug gets its own set of columns.

  rows_with_missing_drug_data_dataset = c()
  for(i in 1:nrow(pData_list[[dataset]])) {
    print(paste0("Adding PROGENy data for row ", i, " for dataset ", dataset, "."))
  
    cell_line = pData_list[[dataset]][i,"cell_line_name"] %>% unlist %>% as.character
    drugs = pData_list[[dataset]][i,"drugs"] %>% str_split("_") %>% unlist %>% as.character
  
    drug_data_mat = matrix(, nrow = length(drugs), ncol = ncol(drug_PROGENy_data[[dataset]]))
    drug_data_2 = c()
    # Make sure all of the (non-null) drugs have PROGENy data.
    num_drugs_in_PROGENy_dataset = drugs %in% rownames(drug_PROGENy_data[[dataset]]) %>% sum
    num_drugs_not_empty = drugs[!(drugs %in% c("NULL", "NA", NULL, NA))] %>% length
    if(num_drugs_in_PROGENy_dataset < num_drugs_not_empty) {
      vector_of_NAs = rep(NA, ncol(drug_PROGENy_data[[dataset]]))
      drug_data_mat[j,] = vector_of_NAs
      drug_data_2 = c(drug_data_2, vector_of_NAs)
      print(paste0("Row ", i, " in the ", dataset, " dataset has one or more drugs with no PROGENy data. Skipping."))
      rows_with_missing_drug_data_dataset = c(rows_with_missing_drug_data_dataset, i)
      next
    }
      
    for(j in 1:length(drugs)) {
      drug = drugs[j]
      drug_data_j = drug_PROGENy_data[[dataset]][rownames(drug_PROGENy_data[[dataset]])==drug,]
      if(drug=="NA" | is.na(drug) | is.null(drug)) {
        vector_of_zeroes = rep(0, ncol(drug_PROGENy_data[[dataset]]))
        drug_data_mat[j,] = vector_of_zeroes
        drug_data_2 = c(drug_data_2, vector_of_zeroes)
      } else {
        drug_data_mat[j,] = drug_data_j
        drug_data_2 = c(drug_data_2, drug_PROGENy_data[[dataset]][rownames(drug_PROGENy_data[[dataset]])==drug,])
      }
      
    }
  
    cell_line_data = cl_PROGENy_data[[dataset]][rownames(cl_PROGENy_data[[dataset]])==cell_line,]
    drug_data_1 = drug_data_mat %>% apply(2, sum, na.rm = T)
    if(length(cell_line_data) < 1) cell_line_data = rep(NA, length(drug_data_1))
  
    descriptor_mat_1[i,] = c(cell_line_data, drug_data_1)
    descriptor_mat_2[i,] = c(cell_line_data, drug_data_2)
    }
  
  colnames(descriptor_mat_1) = c(paste0("cl_", colnames(cl_PROGENy_data[[dataset]])), paste0("drug_", colnames(drug_PROGENy_data[[dataset]]))) # On do.NULL: https://docs.tibco.com/pub/enterprise-runtime-for-R/4.4.0/doc/html/Language_Reference/base/colnames.html
  colnames(descriptor_mat_2) = c(paste0("cl_", colnames(cl_PROGENy_data[[dataset]])), paste0("drug_1_", colnames(drug_PROGENy_data[[dataset]])), paste0("drug_2_", colnames(drug_PROGENy_data[[dataset]])))
  # Filter out the incomplete cases.
  # EDIT 2022-05-27: We will filter out the incomplete cases after adding the metadata/target variables, so we can line up the descriptors and metadata/target variables correctly. 
  #descriptor_mat_1 = descriptor_mat_1 %>% .[complete.cases(.),]
  #descriptor_mat_2 = descriptor_mat_2 %>% .[complete.cases(.),]
  
  # Add to list.
  descriptor_mats[[dataset]][[1]] = descriptor_mat_1
  descriptor_mats[[dataset]][[2]] = descriptor_mat_2
  
  rows_with_missing_drug_data[[dataset]] = rows_with_missing_drug_data_dataset
  
  rm(descriptor_mat_1, descriptor_mat_2)
  gc()
}

# Add metadata and target variable. 
for(dataset in c("ONeil", "ALMANAC")) { 
  descriptor_df_1 = cbind(pData_list[[dataset]], as.data.frame(descriptor_mats[[dataset]][[1]])) %>% .[complete.cases(.),]
  descriptor_df_2 = cbind(pData_list[[dataset]], as.data.frame(descriptor_mats[[dataset]][[2]])) %>% .[complete.cases(.),]
  #descriptor_df_1$Group = descriptor_df_1$Sample %>% as.factor %>% as.numeric
  #descriptor_df_1 = descriptor_df_1 %>% dplyr::relocate(Sample, .before = Drugs)
  #descriptor_df_2 = descriptor_df_2 %>% dplyr::relocate(Sample, .before = Drugs)

  # Save. 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
  write.csv(descriptor_df_1, ml_df_1_filename, row.names = F)
  write.csv(descriptor_df_2, ml_df_2_filename, row.names = F)
}

```

### Spot-checking subset
```{r}
# Take a subset for spot-checking.
subset_proportion = 0.2 # A subset proportion of 20% will give us roughly 460 entries per cell line. If this is further split into training/test at an 80:20 ratio, this will give us 368 observations for the test, 92 for the training. I think that should be reasonable. Still, that's like 28000 rows. If we did 1/5 of that (4% subset), that would give us 74 observations for the test, 18 for the training. Maybe a 5% subset? 

input_filename_list = list()
output_filename_list = list()
for(dataset in c("ONeil", "ALMANAC")) { 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
  
  input_filename_list[[dataset]][["S1"]] = ml_df_1_filename
  input_filename_list[[dataset]][["S2"]] = ml_df_2_filename
  output_filename_list[[dataset]][["S1"]] = ml_df_1_filename %>% regexPipes::gsub("full", "subset")
  output_filename_list[[dataset]][["S2"]] = ml_df_2_filename %>% regexPipes::gsub("full", "subset")

}

for(ds in names(input_filename_list)) {
  for(df in names(input_filename_list[[ds]])) {
    input_filename = input_filename_list[[ds]][[df]]
    output_filename = output_filename_list[[ds]][[df]]
    if(file.exists(output_filename)) {
      print(paste0("The file ", output_filename, " has already been generated. Moving on."))
      next
    }
    
    descriptor_df = read.csv(input_filename, stringsAsFactors = F)
    print(nrow(descriptor_df))
    
    # Rule of thumb: sample size >= 25 should be enough for Pearson's correlation. Similar for Spearman's?
    #https://www.researchgate.net/post/What-is-the-minimum-sample-size-to-run-Pearsons-R
    subset_proportion_final = ifelse(subset_proportion * nrow(descriptor_df) / 60 < 25, 1, subset_proportion)  
    
    # Randomly select treatments (drug combos or single drugs) for each cell line as sample.
    sample_indices = c()
    for(cell_line in unique(descriptor_df$cell_line_name)) {
      # Get all the indices corresponding to entries of that cell line.
      indices = which(descriptor_df$cell_line_name==cell_line)
  
      # Randomly sample from those indices. 
      subset_size = floor(length(indices) * subset_proportion_final)
      set.seed(1026)
      sample_indices = c(sample_indices, base::sample(indices, subset_size))
    }
  
    # Subset using sample_indices.
    descriptor_df_spotchecking_sample = descriptor_df[sample_indices,]
  
    # Save.
    write.csv(descriptor_df_spotchecking_sample, output_filename, row.names = F)
    print(paste0("The number of rows in data frame ", df, " for data set ", ds, " is ", nrow(descriptor_df_spotchecking_sample), "."))
  }
}

rm(descriptor_df_spotchecking_sample)
gc()
```

### Random data
```{r}
input_filename_list = list()
output_filename_list = list()
for(dataset in c("ONeil", "ALMANAC")) { 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
  
  input_filename_list[[dataset]][["S1"]] = ml_df_1_filename
  input_filename_list[[dataset]][["S2"]] = ml_df_2_filename
  output_filename_list[[dataset]][["S1"]] = ml_df_1_filename %>% regexPipes::gsub("full", "subset")
  output_filename_list[[dataset]][["S2"]] = ml_df_2_filename %>% regexPipes::gsub("full", "subset")

}

for(ds in names(input_filename_list)) {
  for(df in names(input_filename_list[[ds]])) {
    additional_info_name = switch(df, 
                                  "S1" = "S1", 
                                  "S2" = "S2", 
                                  "Singles" = "singles",
                                  "Combined" = "combined")
    
    # Load the descriptor data frame.
    subset_descriptor_df_filename = output_filename_list[[ds]][[df]]
    subset_descriptor_df = read.csv(subset_descriptor_df_filename, stringsAsFactors = F)
    
    # Matrix of scrambled descriptors.
    # https://stats.stackexchange.com/a/10229
    set.seed(1026)
    descriptor_mat_rand = subset_descriptor_df %>% .[,28:ncol(.)] %>% as.matrix %>% .[sample.int(nrow(.)),] %>% .[,sample.int(ncol(.))]
    colnames(descriptor_mat_rand) = colnames(subset_descriptor_df)[28:ncol(subset_descriptor_df)]
    # Cbind to form data frame.
    full_descriptor_rand_df = cbind(subset_descriptor_df[,1:27], descriptor_mat_rand)
    full_descriptor_rand_df_filename = filename_generator(data_dir = data_dir, data_source = ds, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = paste0(additional_info_name, "_random"), extension = ".csv")
    write.csv(full_descriptor_rand_df, full_descriptor_rand_df_filename , row.names = F)
    
    print(paste0("The created random inputs file is located at ", full_descriptor_rand_df_filename, "."))
  
  }
  
}
```

# ML model-building
## Spot-checking
### DrugComb
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")

input_filename_list = list()
output_filename_list = list()
for(dataset in c("ONeil", "ALMANAC")) { 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
  
  input_filename_list[[dataset]] = list()
  output_filename_list[[dataset]] = list()
  
  input_filename_list[[dataset]][["S1"]] = ml_df_1_filename
  input_filename_list[[dataset]][["S2"]] = ml_df_2_filename
  output_filename_list[[dataset]][["S1"]] = ml_df_1_filename %>% regexPipes::gsub("full", "subset")
  output_filename_list[[dataset]][["S2"]] = ml_df_2_filename %>% regexPipes::gsub("full", "subset")

}

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC")
target_columns = list() 
target_columns[["ALMANAC"]] = as.integer((16:23)-1)
pred_start_columns = c(as.integer(28-1))
grouping_columns = c(as.integer(5-1))
num_folds = rep(as.integer(5), length(data_sets))

for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in data_sets) { # data_sets
  print(paste0("Working on data set ", ds, "."))

  for(dt in c("S1")) { # names(input_filename_list[[ds]])
    for(target_column in unlist(meta_vars[["target_column"]][[ds]])) {
      # Get the name of the target variable.
      target_var = read.csv(input_filename_list[[ds]][[dt]]) %>% colnames %>% .[(target_column + 1)] # + 1 because we need the R, not Python, numbering. 
      
      timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
      stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_", dt, "_", target_var, "_spotchecking_stdout_", timestamp[1], "_", timestamp[2], ".txt")
      if(!file.exists(stdout_file)) file.create(stdout_file)
      
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_spotchecking_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_spotchecking_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or this data set has been spot-checked already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now spotchecking the data set file ", input_filename, "."))
      print(paste0("The output files will be available at ", output_raw_filename, " and ", output_raw_by_group_filename, "."))
    
      # Make the command.
      command = paste0(
        "python ",
        modeling_functions_dir,
        "model_spot-checking.py -d ",
        input_filename, 
        " -t ", target_column,
        " -s ", meta_vars$pred_start_column[[ds]], 
        " -g ", meta_vars$grouping_column[[ds]],
        " -r ", output_raw_filename, 
        " -b ", output_raw_by_group_filename,
        " -n ", meta_vars$num_folds[[ds]],
        " >> ", stdout_file, " 2>&1"
        )
    
      # Run.
      system(command)
    } # End for() loop looping over the different target columns. 
  } # End for() loop looping over the different descriptor types. 
}
 

# Test.
#system(paste0(
#    "python3 ",
#    modeling_functions_dir,
#    "test.py",
#    " >> ", stdout_file))
```

### Original
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")

ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")

input_filename_list = list()
input_filename_list[["GDSC1"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
input_filename_list[["GDSC2"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "2_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
input_filename_list[["ALMANAC"]] = list()
input_filename_list[["ALMANAC"]][["S1"]] = ALMANAC_PROGENy_ml_df_1_filename %>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["S2"]] = ALMANAC_PROGENy_ml_df_2_filename %>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename %>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename %>% regexPipes::gsub("full", "subset")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC", "GDSC1", "GDSC2")
target_columns = c(as.integer(3-1), as.integer(4-1), as.integer(4-1))
pred_start_columns = c(as.integer(4-1), as.integer(5-1), as.integer(5-1))
grouping_columns = c(as.integer(1-1), as.integer(3-1), as.integer(3-1))
num_folds = rep(as.integer(5), length(data_sets))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

#target_column = as.integer(3 - 1) # 4 - 1 for GDSC.
#pred_start_column = as.integer(4 - 1) # 5 - 1 for GDSC.
#grouping_column = as.integer(1 - 1)
#num_folds = as.integer(5)

for(ds in data_sets) { # data_sets
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_spotchecking_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)
  
  if(class(input_filename_list[[ds]]) != "list") {
    # It's one of the GDSC ones.
    # Check if the input and output file exist.
    # If the input file does not, or if both the output files do, skip.
    input_filename = input_filename_list[[ds]]
    output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("spotchecking_raw_results"), full_path = F))
    output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("spotchecking_raw_results_by_group"), full_path = F))
    if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
      print(paste0("Either the input file ", input_filename, " does not exist or this data set has been spot-checked already. Skipping to the next one."))
      next
    }
    
    print(paste0("Now spotchecking the data set file ", input_filename, "."))
    
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "model_spot-checking.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " >> ", stdout_file, " 2>&1"
      )
    
      # Run.
      system(command)
      
  } else {
    for(dt in c("S1", "S2", "Singles", "Combined")) { # "combined", "1", "2", "singles"
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_spotchecking_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_spotchecking_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or this data set has been spot-checked already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now spotchecking the data set file ", input_filename, "."))
    
      # Make the command.
      command = paste0(
        "python ",
        modeling_functions_dir,
        "model_spot-checking.py -d ",
        input_filename, 
        " -t ", meta_vars$target_column[[ds]],
        " -s ", meta_vars$pred_start_column[[ds]], 
        " -g ", meta_vars$grouping_column[[ds]],
        " -r ", output_raw_filename, 
        " -b ", output_raw_by_group_filename,
        " -n ", meta_vars$num_folds[[ds]],
        " >> ", stdout_file, " 2>&1"
        )
    
      # Run.
      system(command)
    } # End for() loop looping over the different descriptor types. 
  } # End else it's the ALMANAC data set. 
}
 

# Test.
#system(paste0(
#    "python3 ",
#    modeling_functions_dir,
#    "test.py",
#    " >> ", stdout_file))

```

## Hyperparameter tuning (spotchecking)
### DrugComb
```{r}
# We will tune the best-performing model.
# In this case, XGBoost.
# Note that we use the "Spotchecking" input data set to build the model. Even though filenames.R generates a filename for a "Hyperparameter_tuning" input data set, it does not exist. 
# As of 2021-11-13, we will do tuning on the full data set, not the model-spotchecking subset.

reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
input_filename_list = list()
output_filename_list = list()
for(dataset in c("ALMANAC")) { 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  
  input_filename_list[[dataset]] = list()
  output_filename_list[[dataset]] = list()
  
  input_filename_list[[dataset]][["S1"]] = ml_df_1_filename
  output_filename_list[[dataset]][["S1"]] = ml_df_1_filename %>% regexPipes::gsub("full", "subset")

}

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC")
target_columns = list() 
target_columns[["ALMANAC"]] = as.integer((16:23)-1)
pred_start_columns = c(as.integer(28-1))
grouping_columns = c(as.integer(5-1))
num_folds = rep(as.integer(5), length(data_sets))

for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in data_sets) { # data_sets
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_hyperparameter-tuning_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  for(dt in names(input_filename_list[[ds]])) { # "combined", "1", "2", "singles"
    for(target_column in unlist(meta_vars[["target_column"]][[ds]])) {
      # Get the name of the target variable.
      target_var = read.csv(input_filename_list[[ds]][[dt]]) %>% colnames %>% .[(target_column + 1)] # + 1 because we need the R, not Python, numbering. 
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = "", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_full_hyperparameter-tuning_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = "", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_full_hyperparameter-tuning_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or this data set has been hyperparameter-tuned already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now performing hyperparameter tuning for the data set file ", input_filename, "."))
    
      # Make the command.
      command = paste0(
        "python ",
        modeling_functions_dir,
        "hyperparameter_tuning.py -d ",
        input_filename, 
        " -t ", target_column,
        " -s ", meta_vars$pred_start_column[[ds]], 
        " -g ", meta_vars$grouping_column[[ds]],
        " -r ", output_raw_filename, 
        " -b ", output_raw_by_group_filename,
        " -n ", meta_vars$num_folds[[ds]],
        " >> ", stdout_file, " 2>&1"
        )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different target variables. 
  } # End for() loop looping over the different descriptor types. 
}
```

### Original
```{r}
# We will tune the best-performing model.
# In this case, XGBoost.
# Note that we use the "Spotchecking" input data set to build the model. Even though filenames.R generates a filename for a "Hyperparameter_tuning" input data set, it does not exist. 
# As of 2021-11-13, we will do tuning on the full data set, not the model-spotchecking subset.

reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
# Set the file names.
ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")

input_filename_list = list()
input_filename_list[["GDSC1"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", extension = ".csv")
input_filename_list[["GDSC2"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "2_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", extension = ".csv")
input_filename_list[["ALMANAC"]] = list()
input_filename_list[["ALMANAC"]][["S1"]] = ALMANAC_PROGENy_ml_df_1_filename #%>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["S2"]] = ALMANAC_PROGENy_ml_df_2_filename #%>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename #%>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename #%>% regexPipes::gsub("full", "subset")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC", "GDSC1", "GDSC2")
target_columns = c(as.integer(3-1), as.integer(4-1), as.integer(4-1))
pred_start_columns = c(as.integer(4-1), as.integer(5-1), as.integer(5-1))
grouping_columns = c(as.integer(1-1), as.integer(3-1), as.integer(3-1))
num_folds = rep(as.integer(5), length(data_sets))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in c("ALMANAC")) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_hyperparameter-tuning_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  if(class(input_filename_list[[ds]]) != "list") {
    # It's one of the GDSC ones.
    # Check if the input and output file exist.
    # If the input file does not, or if both the output files do, skip.
    input_filename = input_filename_list[[ds]]
    output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = "", data_set = "PROGENy", additional_info = paste0(dt, "_full_hyperparameter-tuning_raw_results"), full_path = F))
    output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = "", data_set = "PROGENy", additional_info = paste0("_full_hyperparameter-tuning_raw_results_by_group"), full_path = F))
    if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
      print(paste0("Either the input file ", input_filename, " does not exist or this data set has been hyperparameter-tuned already. Skipping to the next one."))
      next
    }
    
    print(paste0("Now performing hyperparameter tuning for the data set file ", input_filename, "."))
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "hyperparameter_tuning.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
      
  } else {
    for(dt in c("S1", "S2")) { # "Combined", "S1", "S2", "Singles"
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = "", data_set = "PROGENy", additional_info = paste0(dt, "_full_hyperparameter-tuning_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = "", data_set = "PROGENy", additional_info = paste0(dt, "_full_hyperparameter-tuning_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or this data set has been hyperparameter-tuned already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now performing hyperparameter tuning for the data set file ", input_filename, "."))
    
      # Make the command.
      command = paste0(
        "python ",
        modeling_functions_dir,
        "hyperparameter_tuning.py -d ",
        input_filename, 
        " -t ", meta_vars$target_column[[ds]],
        " -s ", meta_vars$pred_start_column[[ds]], 
        " -g ", meta_vars$grouping_column[[ds]],
        " -r ", output_raw_filename, 
        " -b ", output_raw_by_group_filename,
        " -n ", meta_vars$num_folds[[ds]],
        " >> ", stdout_file, " 2>&1"
        )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different descriptor types. 
  } # End else it's the ALMANAC data set. 
}
```

## Hyperparameter tuning (final model)
### DrugComb
#### Selection of the ranges
```{r}
for(dataset in c("ALMANAC")) { # "ONeil", 
  # Load all the hyperparameter tuning spotchecking results files.
  files = list.files(paste0(aim_dirs$Aim_2$Results, "Machine_learning"), pattern = paste0(dataset, "_PROGENy_DrugComb_.*_full_hyperparameter-tuning_raw_results_by_group"))
  
  for(target_var in c("css_ri", "S_max", "S_mean", "S_sum", "synergy_bliss", "synergy_hsa", "synergy_loewe", "synergy_zip")) {
    files_target_var = files %>% regexPipes::grep(target_var, value = T)
    # Load each data set.
    for(file in files_target_var) {
      dat = read.csv(paste0(aim_dirs$Aim_2$Results, "Machine_learning/", file), stringsAsFactors = F, row.names = 1) %>% .[complete.cases(.),]
      
      # Get the values of the hyperparameters.
      hp_values = rownames(dat) %>% regexPipes::gsub("[[:alpha:]]", "") %>% regexPipes::gsub("[-_]", "") %>% as.numeric # %>% signif(2) not necessary.
    
      # Get the average score under each value of the hyperparameter. 
      avg_score = rowMeans(dat)
      
      # Get the name of the hyperparameter.
      hp_name = rownames(dat)[1] %>% regexPipes::gsub("^[[:alnum:]]+\\-", "") %>% regexPipes::gsub("\\-.+$", "")
      # cbind() and save.
      hp_results = data.frame(Value=hp_values, Score=avg_score)
      
      write.table(hp_results, paste0(aim_dirs$Aim_2$Results, "Machine_learning/", dataset, "_PROGENy_DrugComb_ml_S1_", target_var, "_full_hyperparameter-tuning_results_", hp_name, "_cleaned.csv"), row.names=F, sep = ",")
      
      }
  }
  
}

# The rest of the analysis will be conducted in Prism. 
```

#### Hyperparameter tuning
```{r}
# We will tune the best-performing model.
# In this case, XGBoost.
# Note that we use the "Spotchecking" input data set to build the model. Even though filenames.R generates a filename for a "Hyperparameter_tuning" input data set, it does not exist. 

reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
input_filename_list = list()
output_filename_list = list()
for(dataset in c("ALMANAC")) { 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
  
  input_filename_list[[dataset]] = list()
  output_filename_list[[dataset]] = list()
  
  input_filename_list[[dataset]][["S1"]] = ml_df_1_filename
  input_filename_list[[dataset]][["S2"]] = ml_df_2_filename
  output_filename_list[[dataset]][["S1"]] = ml_df_1_filename %>% regexPipes::gsub("full", "subset")
  output_filename_list[[dataset]][["S2"]] = ml_df_2_filename %>% regexPipes::gsub("full", "subset")

}

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC")
target_columns = list() 
target_columns[["ALMANAC"]] = as.integer((16:23)-1)
pred_start_columns = c(as.integer(28-1))
grouping_columns = c(as.integer(5-1))
num_folds = rep(as.integer(5), length(data_sets))

for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in data_sets) { # data_sets
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_hyperparameter-tuning_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  for(dt in names(input_filename_list[[ds]])) { # "combined", "1", "2", "singles"
    for(target_column in unlist(meta_vars[["target_column"]][[ds]])) {
      # Get the name of the target variable.
      target_var = read.csv(input_filename_list[[ds]][[dt]]) %>% colnames %>% .[(target_column + 1)]
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_hyperparameter-tuning_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_hyperparameter-tuning_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or this data set has been hyperparameter-tuned already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now performing hyperparameter tuning for the data set file ", input_filename, "."))
    
      # Make the command.
      command = paste0(
        "python ",
        modeling_functions_dir,
        "hyperparameter_tuning.py -d ",
        input_filename, 
        " -t ", target_column,
        " -s ", meta_vars$pred_start_column[[ds]], 
        " -g ", meta_vars$grouping_column[[ds]],
        " -r ", output_raw_filename, 
        " -b ", output_raw_by_group_filename,
        " -n ", meta_vars$num_folds[[ds]],
        " -p Complete ",
        " >> ", stdout_file, " 2>&1"
        )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different target variables.  
  } # End for() loop looping over the different descriptor types. 
}
```

### Original
#### Selection of the ranges
```{r}
# Load all the hyperparameter tuning spotchecking results files.
files = list.files(paste0(aim_dirs$Aim_2$Results, "Machine_learning"),  pattern = "ALMANAC_PROGENy_ml_.+_full_hyperparameter-tuning_raw_results_by_group_")

# Load each data set.
for(file in files) {
  dat = read.csv(paste0(aim_dirs$Aim_2$Results, "Machine_learning/", file), stringsAsFactors = F, row.names = 1) %>% .[complete.cases(.),]
  data_set = file %>% str_split("_") %>% unlist %>% .[4]
  
  # Get the values of the hyperparameters.
  hp_values = rownames(dat) %>% regexPipes::gsub("[[:alpha:]]", "") %>% regexPipes::gsub("[-_]", "") %>% as.numeric # %>% signif(2) not necessary.
  
  # Get the average score under each value of the hyperparameter. 
  avg_score = rowMeans(dat)
  
  # Get the name of the hyperparameter. 
  hp_name = rownames(dat)[1] %>% regexPipes::gsub("^[[:alnum:]]+\\-", "") %>% regexPipes::gsub("\\-.+$", "")
  # cbind() and save.
  hp_results = data.frame(Value=hp_values, Score=avg_score)
  
  write.table(hp_results, paste0(aim_dirs$Aim_2$Results, "Machine_learning/", "ALMANAC_PROGENy_ml_", data_set, "_full_hyperparameter-tuning_results_", hp_name, "_cleaned.csv"), row.names=F, sep = ",")
}

# The rest of the analysis will be conducted in Prism. 
```

#### Hyperparameter tuning
```{r}
# We will tune the best-performing model.
# In this case, XGBoost.
# Note that we use the "Spotchecking" input data set to build the model. Even though filenames.R generates a filename for a "Hyperparameter_tuning" input data set, it does not exist. 

reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
# Set the file names.
ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")

input_filename_list = list()
input_filename_list[["GDSC1"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
input_filename_list[["GDSC2"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "2_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
input_filename_list[["ALMANAC"]] = list()
input_filename_list[["ALMANAC"]][["S1"]] = ALMANAC_PROGENy_ml_df_1_filename %>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["S2"]] = ALMANAC_PROGENy_ml_df_2_filename %>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename %>% regexPipes::gsub("full", "subset")
input_filename_list[["ALMANAC"]][["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename %>% regexPipes::gsub("full", "subset")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC", "GDSC1", "GDSC2")
target_columns = c(as.integer(3-1), as.integer(4-1), as.integer(4-1))
pred_start_columns = c(as.integer(4-1), as.integer(5-1), as.integer(5-1))
grouping_columns = c(as.integer(1-1), as.integer(3-1), as.integer(3-1))
num_folds = rep(as.integer(5), length(data_sets))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in c("ALMANAC")) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_hyperparameter-tuning_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  if(class(input_filename_list[[ds]]) != "list") {
    # It's one of the GDSC ones.
    # Check if the input and output file exist.
    # If the input file does not, or if both the output files do, skip.
    input_filename = input_filename_list[[ds]]
    output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_hyperparameter-tuning_raw_results"), full_path = F))
    output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("hyperparameter-tuning_raw_results_by_group"), full_path = F))
    if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
      print(paste0("Either the input file ", input_filename, " does not exist or this data set has been hyperparameter-tuned already. Skipping to the next one."))
      next
    }
    
    print(paste0("Now performing hyperparameter tuning for the data set file ", input_filename, "."))
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "hyperparameter_tuning.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " -p Complete ",
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
      
  } else {
    for(dt in c("Combined")) { # "Combined", "S1", "S2", "Singles"
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_hyperparameter-tuning_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_hyperparameter-tuning_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or this data set has been hyperparameter-tuned already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now performing hyperparameter tuning for the data set file ", input_filename, "."))
    
      # Make the command.
      command = paste0(
        "python ",
        modeling_functions_dir,
        "hyperparameter_tuning.py -d ",
        input_filename, 
        " -t ", meta_vars$target_column[[ds]],
        " -s ", meta_vars$pred_start_column[[ds]], 
        " -g ", meta_vars$grouping_column[[ds]],
        " -r ", output_raw_filename, 
        " -b ", output_raw_by_group_filename,
        " -n ", meta_vars$num_folds[[ds]],
        " -p Complete ",
        " >> ", stdout_file, " 2>&1"
        )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different descriptor types. 
  } # End else it's the ALMANAC data set. 
}
```

## Full-model building
### DrugComb
#### Complete data set
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
input_filename_list = list()
output_filename_list = list()
for(dataset in c("ALMANAC")) { 
  ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
  ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = dataset, data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
  
  input_filename_list[[dataset]] = list()
  output_filename_list[[dataset]] = list()
  
  input_filename_list[[dataset]][["S1"]] = ml_df_1_filename
  input_filename_list[[dataset]][["S2"]] = ml_df_2_filename
  output_filename_list[[dataset]][["S1"]] = ml_df_1_filename %>% regexPipes::gsub("full", "subset")
  output_filename_list[[dataset]][["S2"]] = ml_df_2_filename %>% regexPipes::gsub("full", "subset")

}

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC")
target_columns = list() 
target_columns[["ALMANAC"]] = as.integer((16:23)-1)
pred_start_columns = c(as.integer(28-1))
grouping_columns = c(as.integer(5-1))
num_folds = rep(as.integer(5), length(data_sets))

for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in data_sets) { # data_sets
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_DrugComb_complete-model-building_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  for(dt in names(input_filename_list[[ds]])) { # "combined", "1", "2", "singles"
    for(target_column in unlist(meta_vars[["target_column"]][[ds]])) {
      # Get the name of the target variable.
      target_var = read.csv(input_filename_list[[ds]][[dt]]) %>% colnames %>% .[(target_column + 1)]
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_complete-model-building_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_", target_var, "_complete-model-building_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or a complete model has been built for this data set already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now building the complete model for the data set file ", input_filename, "."))
    
      k_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_full_predictions.csv") # Predictions file.
      c_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_full_test_scores.csv") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv") # c = flag for filename for test scores (evaluation on test split) in full-model_building.py.
    q_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_full_test_scores_by_group.csv") # filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results_by_group %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv")
    f_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_full_complete_model") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("raw_results\\.csv", "") %>% paste0(., "full_model")
    
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "full-model_building.py -d ",
      input_filename, 
      " -t ", target_column,
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " -k ", k_filename,
      " -c ", c_filename,
      " -q ", q_filename,
      " -f ", f_filename,
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different descriptor types. 
  } # End else it's the ALMANAC data set. 

}
```

#### Random data set
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC")
target_columns = list() 
target_columns[["ALMANAC"]] = as.integer((16:23)-1)
pred_start_columns = c(as.integer(28-1))
grouping_columns = c(as.integer(5-1))
num_folds = rep(as.integer(5), length(data_sets))

for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in c("ALMANAC")) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_DrugComb_complete-model-building_random_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)
    for(dt in c("S1")) { # "combined", "S1", "S2", "singles"
      
      additional_info_name = switch(dt, 
                         "S1" = "S1", 
                         "S2" = "S2", 
                         "Singles" = "singles",
                         "Combined" = "combined")
      
      for(target_column in unlist(meta_vars[["target_column"]][[ds]])) {
        # Get the name of the target variable.
        target_var = read.csv(input_filename_list[[ds]][[dt]]) %>% colnames %>% .[(target_column + 1)]
      
        # Get the filename for the random data. 
        # Check if the input and output file exist.
        # If the input file does not, or if both the output files do, skip.
        input_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy_DrugComb", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = paste0(additional_info_name, "_random"), extension = ".csv")
        output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_complete-model-building_random_raw_results"), full_path = F))
        output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy_DrugComb", additional_info = paste0(dt, "_complete-model-building_random_raw_results_by_group"), full_path = F))
        if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
          print(paste0("Either the input file ", input_filename, " does not exist or  a complete model has been built for this data set already. Skipping to the next one."))
          next
        }
        
        print(paste0("Now building the complete model for the data set file ", input_filename, "."))
        
        k_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_random_full_predictions.csv") # Predictions file.
        c_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_random_full_test_scores.csv") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv") # c = flag for filename for test scores (evaluation on test split) in full-model_building.py.
        q_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_random_full_test_scores_by_group.csv") # filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results_by_group %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv")
        f_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_DrugComb_ml_", dt, "_", target_var, "_random_full_complete_model") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("raw_results\\.csv", "") %>% paste0(., "full_model")
        
        # Make the command.
        command = paste0(
          "python ",
          modeling_functions_dir,
          "full-model_building.py -d ",
          input_filename, 
          " -t ", target_column,
          " -s ", meta_vars$pred_start_column[[ds]], 
          " -g ", meta_vars$grouping_column[[ds]],
          " -r ", output_raw_filename, 
          " -b ", output_raw_by_group_filename,
          " -n ", meta_vars$num_folds[[ds]],
          " -k ", k_filename,
          " -c ", c_filename,
          " -q ", q_filename,
          " -f ", f_filename,
          " >> ", stdout_file, " 2>&1"
        )
        
        # Run.
        system(command)
    
    } # End for() loop looping over the different target variables. 
  }  # End for() loop looping over the different descriptor types. 
}
```

### Original
#### Complete data set
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
# Set the file names.
ALMANAC_PROGENy_ml_df_1_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S1", extension = ".csv")
ALMANAC_PROGENy_ml_df_2_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "S2", extension = ".csv")
ALMANAC_PROGENy_ml_df_singles_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "singles", extension = ".csv")
ALMANAC_PROGENy_ml_df_combined_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "combined", extension = ".csv")

input_filename_list = list()
input_filename_list[["GDSC1"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "1_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
input_filename_list[["GDSC2"]] = filename_generator(data_dir = data_dir, data_source = "GDSC", data_set = "2_PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Subset", extension = ".csv")
input_filename_list[["ALMANAC"]] = list()
input_filename_list[["ALMANAC"]][["S1"]] = ALMANAC_PROGENy_ml_df_1_filename
input_filename_list[["ALMANAC"]][["S2"]] = ALMANAC_PROGENy_ml_df_2_filename
input_filename_list[["ALMANAC"]][["Singles"]] = ALMANAC_PROGENy_ml_df_singles_filename 
input_filename_list[["ALMANAC"]][["Combined"]] = ALMANAC_PROGENy_ml_df_combined_filename

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC", "GDSC1", "GDSC2")
target_columns = c(as.integer(3-1), as.integer(4-1), as.integer(4-1))
pred_start_columns = c(as.integer(4-1), as.integer(5-1), as.integer(5-1))
grouping_columns = c(as.integer(1-1), as.integer(3-1), as.integer(3-1))
num_folds = rep(as.integer(5), length(data_sets))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in c("ALMANAC")) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_complete-model-building_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  if(class(input_filename_list[[ds]]) != "list") {
    # It's one of the GDSC ones.
    # Check if the input and output file exist.
    # If the input file does not, or if both the output files do, skip.
    input_filename = input_filename_list[[ds]]
    output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("complete-model-building_raw_results"), full_path = F))
    output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("complete-model-building_raw_results_by_group"), full_path = F))
    if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
      print(paste0("Either the input file ", input_filename, " does not exist or a complete model for this data set has been built already. Skipping to the next one."))
      next
    }
    
    print(paste0("Now building the complete model for the data set file ", input_filename, "."))
    
    k_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_full_predictions.csv") # Predictions file.
    c_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_full_test_scores.csv") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv") # c = flag for filename for test scores (evaluation on test split) in full-model_building.py.
    q_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_full_test_scores_by_group.csv") # filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results_by_group %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv")
    f_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_full_complete_model") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("raw_results\\.csv", "") %>% paste0(., "full_model")
    
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "full-model_building.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " -k ", k_filename,
      " -c ", c_filename,
      " -q ", q_filename,
      " -f ", f_filename,
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
      
  } else {
    for(dt in c("S1", "S2", "Singles", "Combined")) { # "combined", "S1", "S2", "singles"
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = input_filename_list[[ds]][[dt]]
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_complete-model-building_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_complete-model-building_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or a complete model has been built for this data set already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now building the complete model for the data set file ", input_filename, "."))
    
      k_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_full_predictions.csv") # Predictions file.
      c_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_full_test_scores.csv") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv") # c = flag for filename for test scores (evaluation on test split) in full-model_building.py.
    q_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_full_test_scores_by_group.csv") # filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results_by_group %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv")
    f_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_full_complete_model") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("raw_results\\.csv", "") %>% paste0(., "full_model")
    
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "full-model_building.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " -k ", k_filename,
      " -c ", c_filename,
      " -q ", q_filename,
      " -f ", f_filename,
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different descriptor types. 
  } # End else it's the ALMANAC data set. 

}
```

#### Random data set
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["target_column"]] = list()
meta_vars[["pred_start_column"]] = list()
meta_vars[["grouping_column"]] = list()
meta_vars[["num_folds"]] = list()
data_sets = c("ALMANAC", "GDSC1", "GDSC2")
target_columns = c(as.integer(3-1), as.integer(4-1), as.integer(4-1))
pred_start_columns = c(as.integer(4-1), as.integer(5-1), as.integer(5-1))
grouping_columns = c(as.integer(1-1), as.integer(3-1), as.integer(3-1))
num_folds = rep(as.integer(5), length(data_sets))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in c("ALMANAC")) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_complete-model-building_random_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  if(ds != "ALMANAC") {
    # It's one of the GDSC ones.
    # Check if the input and output file exist.
    # If the input file does not, or if both the output files do, skip.
    input_filename = filename_generator(data_dir = data_dir, data_source = ds, data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = paste0(additional_info_name, "_random"), extension = ".csv")
    output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("complete-model-building_random_raw_results"), full_path = F))
    output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("complete-model-building_random_raw_results_by_group"), full_path = F))
    if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
      print(paste0("Either the input file ", input_filename, " does not exist or a complete model for this data set has been built already. Skipping to the next one."))
      next
    }
    
    print(paste0("Now building the complete model for the data set file ", input_filename, "."))
    
    k_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_random_full_predictions.csv") # Predictions file.
    c_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_random_full_test_scores.csv") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv") # c = flag for filename for test scores (evaluation on test split) in full-model_building.py.
    q_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_random_full_test_scores_by_group.csv") # filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results_by_group %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv")
    f_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_random_full_complete_model") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("raw_results\\.csv", "") %>% paste0(., "full_model")
    
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "full-model_building.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " -k ", k_filename,
      " -c ", c_filename,
      " -q ", q_filename,
      " -f ", f_filename,
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
      
  } else {
    for(dt in c("Combined")) { # "combined", "S1", "S2", "singles"
      
      additional_info_name = switch(dt, 
                         "S1" = "S1", 
                         "S2" = "S2", 
                         "Singles" = "singles",
                         "Combined" = "combined")
      
      # Get the filename for the random data. 
      # Check if the input and output file exist.
      # If the input file does not, or if both the output files do, skip.
      input_filename = filename_generator(data_dir = data_dir, data_source = "ALMANAC", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = paste0(additional_info_name, "_random"), extension = ".csv")
      output_raw_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_complete-model-building_random_raw_results"), full_path = F))
      output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0(dt, "_complete-model-building_random_raw_results_by_group"), full_path = F))
      if(!file.exists(input_filename) | (file.exists(output_raw_filename) & file.exists(output_raw_by_group_filename))) {
        print(paste0("Either the input file ", input_filename, " does not exist or  a complete model has been built for this data set already. Skipping to the next one."))
        next
      }
    
      print(paste0("Now building the complete model for the data set file ", input_filename, "."))
    
      k_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_random_full_predictions.csv") # Predictions file.
      c_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_random_full_test_scores.csv") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv") # c = flag for filename for test scores (evaluation on test split) in full-model_building.py.
    q_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_random_full_test_scores_by_group.csv") # filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results_by_group %>% regexPipes::gsub("\\.csv", "") %>% paste0(., "_test_scores.csv")
    f_filename = paste0(aim_dirs$Aim_2$Results, ds, "_PROGENy_ml_", dt, "_random_full_complete_model") #filename_list$MachineLearning[[ds]]$Output[[dt]]$Full$PROGENy$Complete$Raw_results %>% regexPipes::gsub("raw_results\\.csv", "") %>% paste0(., "full_model")
    
    # Make the command.
    command = paste0(
      "python ",
      modeling_functions_dir,
      "full-model_building.py -d ",
      input_filename, 
      " -t ", meta_vars$target_column[[ds]],
      " -s ", meta_vars$pred_start_column[[ds]], 
      " -g ", meta_vars$grouping_column[[ds]],
      " -r ", output_raw_filename, 
      " -b ", output_raw_by_group_filename,
      " -n ", meta_vars$num_folds[[ds]],
      " -k ", k_filename,
      " -c ", c_filename,
      " -q ", q_filename,
      " -f ", f_filename,
      " >> ", stdout_file, " 2>&1"
      )
    
    # Run.
    system(command)
    
    } # End for() loop looping over the different descriptor types. 
  } # End else it's the ALMANAC data set. 

}
```

## Scoring test sets
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
# Set the file names.
# Model filename.
model_filename = paste0(aim_dirs$Aim_2$Results, "Machine_learning/ALMANAC_PROGENy_DrugComb_ml_S1_css_ri_full_complete_model")
# Input filenames.
input_filenames = list()
input_filenames[["ONeil"]] = paste0(folder_generator(data_dir, data_source = "ONeil", data_type = "ML", processing_stage = "Processed"), "ONeil_PROGENy_DrugComb_ml_S1_processed_full.csv")
#input_filenames[["TCGA_surv"]] = paste0(folder_generator(data_dir, data_source = "TCGA", data_type = "ML", processing_stage = "Processed"), "TCGA_survival_PROGENy_ml_processed_full.csv")
#input_filenames[["TCGA_resp"]] = paste0(folder_generator(data_dir = data_dir, data_source = "TCGA", data_type = "ML", processing_stage = "Processed"), "TCGA_response_PROGENy_ml_processed_combos_full.csv") #paste0(folder_generator(data_dir, data_source = "TCGA", data_type = "ML", processing_stage = "Processed"), "TCGA_response_PROGENy_ml_processed_full.csv")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["pred_start_column"]] = list()
data_sets = c("ONeil") # , "TCGA_surv", "TCGA_resp"
pred_start_columns = c(as.integer(28-1), as.integer(5-1), as.integer(5-1))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in names(input_filenames)) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_scoring_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  # Check if the input and output file exist.
  # If the input file does not, or if both the output files do, skip.
  input_filename = input_filenames[[ds]]
  data_source = ds %>% regexPipes::gsub("_.+$", "")
  folder = paste0(folder_generator(data_dir, data_source = data_source, data_type = "ML", processing_stage = "Processed"))
  output_filename = input_filename %>% regexPipes::gsub(folder, "") %>% regexPipes::gsub("\\.csv", "_scored.csv")
  output_file_path = paste0(aim_dirs$Aim_2$Results, output_filename)
  if(!file.exists(input_filename) | file.exists(output_file_path)) {
    print(paste0("Either the input file ", input_filename, " does not exist or this data set has been scored already. Skipping to the next one."))
    next
  }
    
  print(paste0("Now scoring the data set file ", input_filename, "."))
    
  # Make the command.
  command = paste0(
    "python ",
    modeling_functions_dir,
    "scoring.py -d ",
    input_filename, 
    " -s ", meta_vars$pred_start_column[[ds]], 
    " -m ", model_filename,
    " -p ", output_file_path,
    " >> ", stdout_file, " 2>&1"
    )
    
  # Run.
  system(command)

}
```

## Testing on clinical data
### TCGA
```{r}
reticulate::conda_list()
use_condaenv("py3.8", required = TRUE)
py_config()
system("which python")

modeling_functions_dir = paste0(script_dir, "Machine_learning/")
  
# Set the file names.
# Model filename.
model_filename = paste0(aim_dirs$Aim_2$Results, "Machine_learning/ALMANAC_PROGENy_ml_Combined_full_complete_model")
# Input filenames.
input_filenames = list()
input_filenames[["TCGA"]] = filename_generator(data_dir = data_dir, data_source = "TCGA", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "Combos", extension = ".csv")

# Set the variables. 
# Remember that Python numbering starts at 0, unlike R, which starts at 1, so we will need to subtract 1 from whatever index we would use in R to get the equivalent Python index.
# Also, apparently numbers in R are sent over as type float by default, even the ones that look like integers. So we will have to explicitly convert to integers before calling functions in Python.
meta_vars = list()
meta_vars[["pred_start_column"]] = list()
data_sets = c("TCGA")
pred_start_columns = c(as.integer(3-1))
for(i in 1:length(data_sets)) {
  data_set = data_sets[i]
  for(j in 1:length(names(meta_vars))) {
    meta_var = names(meta_vars)[j]
    
    meta_var_name = ifelse(meta_var=="num_folds", meta_var, paste0(meta_var, "s"))
    meta_vars[[meta_var]][[data_set]] = get(meta_var_name)[i] # get(): https://stackoverflow.com/a/10430015
  }
}

for(ds in names(input_filenames)) {
  print(paste0("Working on data set ", ds, "."))
  
  timestamp = Sys.time() %>% str_split(" ") %>% unlist %>% regexPipes::gsub(":", "-")
stdout_file = paste0(aim_dirs$Aim_2$Results, ds, "_scoring_stdout_", timestamp[1], "_", timestamp[2], ".txt")
  if(!file.exists(stdout_file)) file.create(stdout_file)

  # Check if the input and output file exist.
  # If the input file does not, or if both the output files do, skip.
  input_filename = input_filenames[[ds]]
  data_source = ds %>% regexPipes::gsub("_.+$", "")
  folder = paste0(folder_generator(data_dir, data_source = data_source, data_type = "ML", processing_stage = "Processed"))
  output_filename = input_filename %>% regexPipes::gsub(folder, "") %>% regexPipes::gsub("\\.csv", "_scored.csv")
  output_file_path = paste0(aim_dirs$Aim_2$Results, output_filename)
  if(!file.exists(input_filename) | file.exists(output_file_path)) {
    print(paste0("Either the input file ", input_filename, " does not exist or this data set has been scored already. Skipping to the next one."))
    next
  }
    
  print(paste0("Now scoring the data set file ", input_filename, "."))
    
  # Make the command.
  command = paste0(
    "python ",
    modeling_functions_dir,
    "scoring.py -d ",
    input_filename, 
    " -s ", meta_vars$pred_start_column[[ds]], 
    " -m ", model_filename,
    " -p ", output_file_path,
    " >> ", stdout_file, " 2>&1"
    )
    
  # Run.
  system(command)

}

```

# Results
## Descriptive statistics
### Examination of concentrations (Fig. S8)
Xia et al. claim that the concentrations used by ALMANAC by and large fall within the limits of "FDA-approved clinical doses." Let's examine that claim for ourselves. For what constitutes an "FDA-approved clinical dose," we will look to "Clinically Relevant Concentrations of Anticancer Drugs: A Guide for Nonclinical Studies," by Liston and Davis.
#### ALMANAC
```{r}
pData = readRDS(filename_generator(data_dir, data_source = "ALMANAC", data_type = "pData", extension = ".rds", processing_stage = "Processed"))
library(readxl)
liston_davis = read_excel(paste0(folder_generator(data_dir, data_source = "Liston_and_Davis_Clinical_Cancer_Res_2017", processing_stage = "Raw", data_type = "Drugs"), "174902_1_supp_3907792_qmrtks.xlsx"))
cmax = liston_davis$`Cmax (uM)` 
cmax = as.numeric(cmax[!(cmax %in% c("n.d.", "BLOQ"))]) # In uM.
hist(log10(cmax))
range(cmax)
median(cmax)

sum(pData$CONC1 > (median(cmax*10^-6))) / length(pData$CONC1)
median(pData$CONC1) < median(cmax*10^-6)

# Create the data table.
almanac_conc = pData$CONC1 %>% unique
dat = data.frame(
  Concentration = c(almanac_conc/10^-6, cmax),
  Source = c(rep("ALMANAC", length(almanac_conc)), rep("FDA", length(cmax)))
)
# Perform Mann-Whitney U test to assess statistical significance.
wilcox.test(Concentration ~ Source, data = dat)

# Histogram: number of nodes per module.
col_name = "Concentration"
freqpoly = dat %>% ggplot(aes(x = get(col_name), color = Source, after_stat(density))) + 
  geom_freqpoly(aes(fill=Source), alpha=0.5, position="identity", binwidth = 100, size = 1.5) + 
  scale_color_manual(values=bar_colors) +  
  scale_fill_manual(values=bar_colors) + 
  theme_classic() + 
  theme(text=element_text(family = font, face = "bold"),
        plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
        axis.line = element_line(size = axis_line_size),
        axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
        axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
        axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
        axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
        axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
        axis.ticks = element_line(size = axis_thickness) # We want tick size to be the same as the axis size. 
        ) +
           #legend.title = element_text(size = legend_title_size),
           #legend.text = element_text(size = legend_text_size)) +
        labs(title="Comparison of concentrations of drugs used in ALMANAC vs clinically approved concentrations", x="Concentration (M)", y="Density")

# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/FigureS8.png"), width = 1250, height = 800)
print(freqpoly)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/FigureS8.eps"), width = 25, height = 25)
histogram
dev.off()
```

#### O'Neil
```{r}
pData = readRDS(filename_generator(data_dir, data_source = "ONeil", data_type = "pData", extension = ".rds", processing_stage = "Processed"))
library(readxl)
liston_davis = read_excel(paste0(folder_generator(data_dir, data_source = "Liston_and_Davis_Clinical_Cancer_Res_2017", processing_stage = "Raw", data_type = "Metadata"), "174902_1_supp_3907792_qmrtks.xlsx"))
cmax = liston_davis$`Cmax (uM)` 
cmax = as.numeric(cmax[!(cmax %in% c("n.d.", "BLOQ"))]) # In uM.
hist(log10(cmax))
range(cmax)
median(cmax)

sum(pData$CONC1 > (median(cmax*10^-6))) / length(pData$CONC1)
median(pData$CONC1) < median(cmax*10^-6)
```

## Comparison of the machine-learning models
### Spot-checking results
#### Data
```{r}
# List all the results files.
filenames = list.files(path = paste0(aim_dirs$Aim_2$Results, "Machine_learning"), pattern = "spotchecking_raw_results_by_group\\.csv")

for(filename in filenames) {
  # Load the file.
  results = read.table(paste0(aim_dirs$Aim_2$Results, "Machine_learning/", filename), row.names = 1, sep = ",", header = T)
  colnames(results) = colnames(results) %>% regexPipes::gsub("\\.", "-") %>% regexPipes::gsub("X786", "786")
  
  # Arrange the columns.
  results = results[,order(colnames(results))]
  
  # Add a column for the average.
  results$Mean = rowMeans(results, na.rm = T)
  
  # Sort by average, descending order.
  results = results %>% dplyr::arrange(-Mean)
  
  # Save the file. 
  output_filename = filename %>% regexPipes::gsub("\\.csv", "_for_Prism.csv")
  output_file_path = paste0(aim_dirs$Aim_2$Results, "Machine_learning/", output_filename)
  write.table(results, output_file_path, sep = ",", row.names = T, col.names = T)
}
```

#### Graphs and tables (Fig. 7a, File S6, Fig. S9)
```{r}
# List all the results files.
filenames = list.files(path = paste0(aim_dirs$Aim_2$Results, "Machine_learning"), pattern = "spotchecking_raw_results_by_group\\.csv")
# Functions for use with select_if() in for() loop. https://stackoverflow.com/a/50335187
not_all_na = function(x) any(!is.na(x))
not_any_na = function(x) all(!is.na(x))

plots_7a = list()
plots_S9 = list()
# 7a
for(filename in filenames[9]) { # ALMANAC_PROGENy_ml_Combined_spotchecking_raw_results_by_group.csv
  # Load the file.
  results = read.table(paste0(aim_dirs$Aim_2$Results, "Machine_learning/", filename), row.names = 1, sep = ",", header = T) %>% dplyr::select_if(not_all_na) %>% .[complete.cases(.),] # The select_if() part gets rid of any columns with all NAs.
  colnames(results) = colnames(results) %>% regexPipes::gsub("\\.", "-") %>% regexPipes::gsub("X786", "786")
  
  # Arrange the columns.
  #results = results[,order(colnames(results))]
  
  # Add a column for the average.
  #results$Mean = rowMeans(results, na.rm = T)
  
  # Sort by average, descending order.
  #results = results %>% dplyr::arrange(-Mean)
  
  # Graph. 
  results_sub = results
  results_sub$Mean = rowMeans(results_sub, na.rm = T)
  results_sub = results_sub %>% dplyr::arrange(-Mean) %>% .[1:5,]
  results_sub$Model = rownames(results_sub)
  results_sub_melted = melt(results_sub)
  colnames(results_sub_melted) = c("Model", "CellLine", "Correlation")
  
  filename_elements = filename %>% str_split("_") %>% unlist
  if(filename_elements[1]=="ALMANAC") {
    graph_title = paste(filename_elements[1], filename_elements[4])
  } else {
    graph_title = paste(filename_elements[1])
  }
  
  # Use ggplot2 to create a bar chart + dotplot. 
  plots_7a[[filename]] = local({ # https://stackoverflow.com/a/31994539
    results_sub_melted = results_sub_melted
    
    g = ggplot(data=results_sub_melted, aes(x=Model, y=Correlation, fill=Model)) +
     geom_bar(position = "dodge", stat = "summary", fun = 'mean', alpha = 0.5) +
     scale_fill_manual(values=ml_colors) + 
     geom_point(aes(x = Model, y = Correlation, color = Model), size = dot_size, position = position_jitter(w = 0.1, h = 0)) + # https://stackoverflow.com/a/31406125
     scale_color_manual(values=ml_colors) + 
     stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="errorbar", color="black", size = error_bar_thickness, width=0.2, position = position_dodge(width = 1)) +
     theme_classic() + 
     theme(text=element_text(family = font, face = "bold"),
           plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
           axis.line = element_line(size = axis_line_size),
           axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
           axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
           axis.ticks.x = element_blank(),
           axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
           axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
           axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
           axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
           legend.position = "none") +
           #legend.title = element_text(size = legend_title_size),
           #legend.text = element_text(size = legend_text_size)) +
     labs(title="Spotchecking for machine-learning model selection", x=" ", y=" ") + 
    stat_compare_means(method = "anova",
                       size = p_value_size,
                       label.y = 1.05,
                       #label =  "p.signif", 
                       #label.x = 1.5, 
                       #symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1),
                       #                   symbols = c("****", "***", "**", "*", " "))
      )
    print(g)
  })  
  
  # Save results to table. 
  xl_filename = filename %>% regexPipes::gsub("\\..+$", "")
  write.xlsx(results, file = paste0(aim_dirs$Aim_2$Results, "Machine_learning/", xl_filename, ".xlsx"), sheetName = "xl_filename", append = FALSE)
  
}

# S9
for(filename in filenames[10:12]) {
  # Load the file.
  results = read.table(paste0(aim_dirs$Aim_2$Results, "Machine_learning/", filename), row.names = 1, sep = ",", header = T) %>% dplyr::select_if(not_all_na) %>% .[complete.cases(.),] # The select_if() part gets rid of any columns with all NAs.
  colnames(results) = colnames(results) %>% regexPipes::gsub("\\.", "-") %>% regexPipes::gsub("X786", "786")
  
  # Arrange the columns.
  #results = results[,order(colnames(results))]
  
  # Add a column for the average.
  #results$Mean = rowMeans(results, na.rm = T)
  
  # Sort by average, descending order.
  #results = results %>% dplyr::arrange(-Mean)
  
  # Graph. 
  results_sub = results
  results_sub$Mean = rowMeans(results_sub, na.rm = T)
  results_sub = results_sub %>% dplyr::arrange(-Mean) %>% .[1:5,]
  results_sub$Model = rownames(results_sub)
  results_sub_melted = melt(results_sub)
  colnames(results_sub_melted) = c("Model", "CellLine", "Correlation")
  
  filename_elements = filename %>% str_split("_") %>% unlist
  if(filename_elements[1]=="ALMANAC") {
    graph_title = paste(filename_elements[1], filename_elements[4])
  } else {
    graph_title = paste(filename_elements[1])
  }
  
  # Use ggplot2 to create a bar chart + dotplot. 
  plots_S9[[filename]] = local({ # https://stackoverflow.com/a/31994539
    results_sub_melted = results_sub_melted
    
    g = ggplot(data=results_sub_melted, aes(x=Model, y=Correlation, fill=Model)) +
     geom_bar(position = "dodge", stat = "summary", fun = 'mean', alpha = 0.5) +
     scale_fill_manual(values=ml_colors) + 
     geom_point(aes(x = Model, y = Correlation, color = Model), size = dot_size, position = position_jitter(w = 0.1, h = 0)) + # https://stackoverflow.com/a/31406125
     scale_color_manual(values=ml_colors) + 
     stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="errorbar", color="black", size = error_bar_thickness, width=0.2, position = position_dodge(width = 1)) +
     theme_classic() + 
     theme(text=element_text(family = font, face = "bold"),
           plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
           axis.line = element_line(size = axis_line_size),
           axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
           axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
           axis.ticks.x = element_blank(),
           axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
           axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
           axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
           axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
           legend.position = "none") +
           #legend.title = element_text(size = legend_title_size),
           #legend.text = element_text(size = legend_text_size)) +
     labs(title=graph_title, x=" ", y=" ") #+ 
    #stat_compare_means(label =  "p.signif", 
    #                   label.x = 1.5, 
    #                   symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1),
    #                                      symbols = c("****", "***", "**", "*", " ")), 
    #                   size = p_value_size ) 
    print(g)
  })  
  
  # Save results to table. 
  xl_filename = filename %>% regexPipes::gsub("\\..+$", "")
  write.xlsx(results, file = paste0(aim_dirs$Aim_2$Results, "Machine_learning/", xl_filename, ".xlsx"), sheetName = "xl_filename", append = FALSE)
  
}

# Arrange plots.
figure_7a = ggpubr::ggarrange(plotlist = plots_7a, 
                   nrow = 1, 
                   ncol = 1, 
                   labels = letters[1],
                   hjust = c(-0.5, -1, -0.5, -1, -1.5),
                   font.label = list(size = 20)
                   #common.legend = TRUE, 
                   #legend = "right"
                   )
figure_S9 = ggpubr::ggarrange(plotlist = plots_S9, 
                   nrow = 2, 
                   ncol = 2, 
                   labels = letters[1:length(plots_S9)],
                   hjust = c(-0.5, -1, -0.5, -1, -1.5),
                   font.label = list(size = 20)
                   #common.legend = TRUE, 
                   #legend = "right"
                   )

# Annotate figure by adding a common label.
# https://github.com/kassambara/ggpubr/issues/78
figure_7a_annotated = annotate_figure(figure_7a,
                                   top = text_grob(paste(" "), size = overall_title_size),
                                   left = text_grob(paste("Spearman's correlation coefficient"), rot = 90, size = common_axis_text_y_size),
                                   bottom = text_grob(paste("Model"), size = common_axis_text_x_size),
                                   fig.lab = " ", 
                                   fig.lab.size = figure_label_size,
                                   fig.lab.face = figure_label_face
)
figure_S9_annotated = annotate_figure(figure_S9,
                                   top = text_grob(paste("Spotchecking for machine-learning model selection"), size = overall_title_size),
                                   left = text_grob(paste("Spearman's correlation coefficient"), rot = 90, size = common_axis_text_y_size),
                                   bottom = text_grob(paste("Model"), size = common_axis_text_x_size),
                                   fig.lab = " ", 
                                   fig.lab.size = figure_label_size,
                                   fig.lab.face = figure_label_face
)

# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7a.png"), width = 1000, height = 1000)
print(figure_7a_annotated)
dev.off()
png(paste0(aim_dirs$Aim_2$Results, "Figures/FigureS9.png"), width = 3000, height = 2500)
print(figure_S9_annotated)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7a.eps"), width = 25, height = 25)
figure_7a_annotated
dev.off()
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/FigureS9.eps"), width = 25, height = 25)
figure_S9_annotated
dev.off()
```

### Hyperparameter tuning results
#### Data
```{r}
# Load the data. 
ds = "ALMANAC"
output_raw_by_group_filename = paste0(aim_dirs$Aim_2$Results, filename_generator(data_dir, data_source = ds, data_type = "ML", extension = ".csv", data_set = "PROGENy", additional_info = paste0("Combined_hyperparameter-tuning_raw_results_by_group"), full_path = F))
results = read.csv(output_raw_by_group_filename, row.names = 1)

# Clean up.
colnames(results) = colnames(results) %>% regexPipes::gsub("\\.", "-") # Clean up column names.
results = results[complete.cases(results),] # Remove NAs.

# Keep only the top 5 by average.
results_top_5 = results %>% 
  dplyr::mutate(Mean = rowMeans(.)) %>% # Add a column containing the mean for each row.
  dplyr::arrange(desc(Mean)) %>% # Arrange in descending order of mean.
  .[1:5,] %>% # Keep only the top 5 by mean.
  dplyr::select(-Mean) # Don't need the Mean column anymore, so remove it. 

# Get into proper form for graphing. 
results_top_5_melted = results_top_5 %>% t %>% melt() # Melt. 
parameter_sets = results_top_5_melted$Var2 # Save the values of the parameter sets.
results_top_5_melted$Var2 = results_top_5_melted$Var2 %>% as.factor %>% as.numeric 
results_top_5_melted$Var2 = paste("Set", results_top_5_melted$Var2)

# Graph.
results_top_5_melted = results_top_5_melted %>% ggplot(aes(x = Var2, y = value)) +
  geom_bar(stat = "summary", fun.y = "mean")
```

#### Graphs and tables (Fig. S10, 7b - c)
```{r}
######################################
## INDIVIDUAL HYPERPARAMETER TUNING ##
######################################

# List all the results files.
filenames = list.files(path = paste0(aim_dirs$Aim_2$Results, "Machine_learning"), pattern = "Combined_full_hyperparameter-tuning_raw_results_by_group")

# Cleaned-up hyperparameter names.
a = c("colsample_bytree", "learning_rate", "max_depth", "min_child_weight", "subsample")
b = c("Proportion of features used to build trees", "Learning rate", "Depth of each tree", "Minimum node sample size", "Subsample ratio of training instances")
names(b) = a

# Fig. S10
plots = list()
for(filename in filenames) {
  # Load the file.
  results = read.table(paste0(aim_dirs$Aim_2$Results, "Machine_learning/", filename), row.names = 1, sep = ",", header = T) %>% .[complete.cases(.),]
  colnames(results) = colnames(results) %>% regexPipes::gsub("\\.", "-") %>% regexPipes::gsub("X786", "786")
  
  # Add a column for the average.
  results$Mean = rowMeans(results, na.rm = T)
  
  # Create a new data frame with two columns: 1) the values of the hyperparameter and 2) the corresponding (mean) accuracy. 
  dat = data.frame(
    HyperparameterValues = rownames(results) %>% regexPipes::gsub("[-a-zA-z_]", "") %>% as.numeric() %>% round(digits = 2),
    CorrelationCoefficient = results$Mean #%>% round(digits = 2)
  )
  
  # Graph. 
  hyperparameter = filename %>% regexPipes::gsub("ALMANAC_PROGENy_ml_Combined_full_hyperparameter-tuning_raw_results_by_group_", "") %>% regexPipes::gsub("\\.csv", "") 
  hyperparameter_cleaned = b[names(b)==hyperparameter]
  
  # Use ggplot2 to create a bar chart + dotplot. 
  plots[[filename]] = local({ # https://stackoverflow.com/a/31994539
    dat = dat
    
    g = ggplot(data=dat, aes(x=HyperparameterValues, y=CorrelationCoefficient)) +
     geom_line(color = "grey") +
     geom_point(shape = 21, color = "black", fill = "#69b3a2", size = 6) +
     theme_classic() + 
      ylim(0.6, 0.8) + 
     theme(text=element_text(family = font, face = "bold"),
           plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
           axis.line = element_line(size = axis_line_size),
           axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
           axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
           axis.ticks.x = element_blank(),
           axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
           axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
           axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
           axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
           legend.position = "none") +
           #legend.title = element_text(size = legend_title_size),
           #legend.text = element_text(size = legend_text_size)) +
     labs(title=" ", x=hyperparameter_cleaned, y=" ") #+ 
    #stat_compare_means(label =  "p.signif", 
    #                   label.x = 1.5, 
    #                   symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1),
    #                                      symbols = c("****", "***", "**", "*", " ")), 
    #                   size = p_value_size ) 
    print(g)
  })  
  
  # Save results to table. 
  #xl_filename = filename %>% regexPipes::gsub("\\..+$", "")
  #write.xlsx(results, file = paste0(aim_dirs$Aim_2$Results, "Machine_learning/", xl_filename, ".xlsx"), sheetName = "xl_filename", append = FALSE)
  
}

# Arrange plots.
figure = ggpubr::ggarrange(plotlist = plots, 
                   nrow = 2, 
                   ncol = 3, 
                   labels = letters[1:length(plots)],
                   hjust = c(-0.5, -1, -0.5, -1, -1.5),
                   font.label = list(size = 20),
                   common.legend = TRUE, 
                   legend = "right")
# Annotate figure by adding a common label.
# https://github.com/kassambara/ggpubr/issues/78
figure_annotated = annotate_figure(figure,
                                   top = text_grob(paste("Tuning of individual hyperparameters for XGBoost"), size = overall_title_size),
                                   left = text_grob(paste("Spearman's correlation coefficient"), rot = 90, size = common_axis_text_y_size),
                                   bottom = text_grob(paste(""), size = common_axis_text_x_size),
                                   fig.lab = " ", 
                                   fig.lab.size = figure_label_size,
                                   fig.lab.face = figure_label_face
)

# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/FigureS10.png"), width = 2500, height = 2000)
print(figure_annotated)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/FigureS10.eps"), width = 25, height = 25)
figure_annotated
dev.off()

################################
## HYPERPARAMETER GRID TUNING ##
################################

filename = paste0(aim_dirs$Aim_2$Results, "Machine_learning/ALMANAC_PROGENy_ml_Combined_hyperparameter-tuning_raw_results_by_group.csv")
results = read.csv(filename, row.names = 1) %>% .[complete.cases(.),]

# Save the results to Excel.
results_xl = results
colnames(results_xl) = colnames(results_xl) %>% regexPipes::gsub("\\.", "\\-")
hps_vec = c()
for(i in 1:nrow(results_xl)) {
  int_hps = rownames(results_xl)[i] %>% str_split("-") %>% unlist %>% .[2] %>% as.integer
  num_hps = rownames(results_xl)[i] %>% str_split("-") %>% unlist %>% .[3:5] %>% as.numeric %>% round(2)
  hps = paste0("XGB-", paste(c(int_hps, num_hps), collapse="-"))
  
  hps_vec = c(hps_vec, hps)
}
rownames(results_xl) = hps_vec
write.xlsx(results_xl, file = paste0(aim_dirs$Aim_2$Results, "Machine_learning/File_S7.xlsx"), sheetName = "Hyperparameter_tuning", append = FALSE)

# Keep only the top 5 by average.
results_top_5 = results %>% 
  dplyr::mutate(Mean = rowMeans(.)) %>% # Add a column containing the mean for each row.
  dplyr::arrange(desc(Mean)) %>% # Arrange in descending order of mean.
  .[1:5,] %>% # Keep only the top 5 by mean.
  dplyr::select(-Mean) %>% 
  .[,order(colnames(.))] # Don't need the Mean column anymore, so remove it. 
results_top_5$Group = paste("Hyperparameter set", 1:5)
# Add the original (untuned) model. 
original_model_filename = paste0(aim_dirs$Aim_2$Results, "ALMANAC_PROGENy_ml_Combined_complete-model-building_raw_results_by_group.csv")
original_model_results = read.csv(original_model_filename, row.names = 1) %>% .[,order(colnames(.))]
#rowMeans(original_model_results)
results_top_5 = rbind(c(original_model_results[1,], Group="Original"), results_top_5)
results_top_5_melted = melt(results_top_5)
colnames(results_top_5_melted) = c("Group", "CellLine", "Correlation")
results_top_5_melted$Group = factor(results_top_5_melted$Group, levels = c("Original", paste("Hyperparameter set", 1:5)))

# Graph.
g = ggplot(data=results_top_5_melted, aes(x=Group, y=Correlation, fill=Group)) +
  geom_bar(position = "dodge", stat = "summary", fun = 'mean', alpha = 0.5) +
  scale_fill_manual(values=RColorBrewer::brewer.pal(6, "Set2")) + 
  geom_point(aes(x = Group, y = Correlation, color = Group), size = dot_size, position = position_jitter(w = 0.1, h = 0)) + # https://stackoverflow.com/a/31406125
  scale_color_manual(values=RColorBrewer::brewer.pal(6, "Set2")) + 
  stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="errorbar", color="black", size = error_bar_thickness, width=0.2, position = position_dodge(width = 1)) +
  theme_classic() + 
  theme(text=element_text(family = font, face = "bold"),
        plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
        axis.line = element_line(size = axis_line_size),
        axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
        axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
        axis.ticks.x = element_blank(),
        axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
        axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
        axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
        axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
        legend.position = "none") +
        #legend.title = element_text(size = legend_title_size),
        #legend.text = element_text(size = legend_text_size)) +
        labs(title="Results of hyperparameter tuning by random grid search", x=" ", y="Spearman's correlation coefficient") + 
    stat_compare_means(method = "anova",
                       size = p_value_size,
                       label.y = 0.9,
                       #label =  "p.signif", 
                       #label.x = 1.5, 
                       #symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1),
                       #                   symbols = c("****", "***", "**", "*", " "))
      )

figure_annotated = annotate_figure(g,
                                  #top = text_grob(paste(), size = overall_title_size),
                                  #left = text_grob(paste(), rot = 90, size = common_axis_text_y_size),
                                  #bottom = text_grob(paste(), size = common_axis_text_x_size),
                                  fig.lab = letters[2],
                                  fig.lab.size = figure_label_size,
                                  fig.lab.face = figure_label_face
                                  )
# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7b.png"), width = 1500, height = 1500)
print(figure_annotated)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7b.eps", width = 25, height = 25))
figure_annotated
dev.off()

###########################################
## COMPARISON OF FINAL MODEL WITH RANDOM ##
###########################################

random_results_filename = paste0(aim_dirs$Aim_2$Results, "Machine_learning/ALMANAC_PROGENy_ml_Combined_complete-model-building_random_raw_results_by_group.csv")
random_results = read.csv(random_results_filename, row.names = 1) %>% .[,order(colnames(.))]

dat = rbind(rbind(as.data.frame(c(original_model_results[1,], Group="Trained model")), as.data.frame(c(random_results[1,], Group="Random"))))
dat_melted = melt(dat)
colnames(dat_melted) = c("Group", "CellLine", "Correlation")

# Graph.
g = ggplot(data=dat_melted, aes(x=Group, y=Correlation, fill=Group)) +
  geom_bar(position = "dodge", stat = "summary", fun = 'mean', alpha = 0.5) +
  scale_fill_manual(values=RColorBrewer::brewer.pal(2, "Set3")) + 
  geom_point(aes(x = Group, y = Correlation, color = Group), size = dot_size, position = position_jitter(w = 0.1, h = 0)) + # https://stackoverflow.com/a/31406125
  scale_color_manual(values=RColorBrewer::brewer.pal(2, "Set3")) + 
  stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="errorbar", color="black", size = error_bar_thickness, width=0.2, position = position_dodge(width = 1)) +
  theme_classic() + 
  theme(text=element_text(family = font, face = "bold"),
        plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
        axis.line = element_line(size = axis_line_size),
        axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
        axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
        axis.ticks.x = element_blank(),
        axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
        axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
        axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
        axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
        legend.position = "none") +
        #legend.title = element_text(size = legend_title_size),
        #legend.text = element_text(size = legend_text_size)) +
        labs(title=" ", x=" ", y="Spearman's correlation coefficient") + 
        stat_compare_means(method = "t.test",
                       size = p_value_size,
                       label.y = 0.95,
                       #label =  "p.signif", 
                       #label.x = 1.5, 
                       #symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1),
                       #                   symbols = c("****", "***", "**", "*", " "))
      )

figure_annotated = annotate_figure(g,
                                  #top = text_grob(paste(), size = overall_title_size),
                                  #left = text_grob(paste(), rot = 90, size = common_axis_text_y_size),
                                  #bottom = text_grob(paste(), size = common_axis_text_x_size),
                                  fig.lab = letters[3],
                                  fig.lab.size = figure_label_size,
                                  fig.lab.face = figure_label_face
                                  )

# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7c.png"), width = 800, height = 800)
print(figure_annotated)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7c.eps"), width = 25, height = 25)
figure_annotated
dev.off()

```

## Comparison of drug-descriptor methods
### Graphing (Fig. 7d)
```{r}
# Load the data.
S1_results_filename = paste0(aim_dirs$Aim_2$Results, "Machine_learning/ALMANAC_PROGENy_ml_S1_complete-model-building_raw_results_by_group.csv")
S2_results_filename = paste0(aim_dirs$Aim_2$Results, "Machine_learning/ALMANAC_PROGENy_ml_S2_complete-model-building_raw_results_by_group.csv")
S1_results = read.csv(S1_results_filename, row.names = 1)
S2_results = read.csv(S2_results_filename, row.names = 1)

# Create the data frame for graphing. 
dat = data.frame(
  Group = c(rep("S1", ncol(S1_results)), rep("S2", ncol(S2_results))),
  Correlation = c(S1_results[1,] %>% unlist, S2_results[1,] %>% unlist)
)

g = ggplot(data=dat, aes(x=Group, y=Correlation, fill=Group)) +
     geom_bar(position = "dodge", stat = "summary", fun = 'mean', alpha = 0.5) +
     scale_fill_manual(values=ml_colors) + 
     geom_point(aes(x = Group, y = Correlation, color = Group), size = dot_size, position = position_jitter(w = 0.1, h = 0)) + # https://stackoverflow.com/a/31406125
     scale_color_manual(values=ml_colors) + 
     stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="errorbar", color="black", size = error_bar_thickness, width=0.2, position = position_dodge(width = 1)) +
     theme_classic() + 
     theme(text=element_text(family = font, face = "bold"),
           plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
           axis.line = element_line(size = axis_line_size),
           axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
           axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
           axis.ticks.x = element_blank(),
           axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
           axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
           axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
           axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
           legend.position = "none") +
           #legend.title = element_text(size = legend_title_size),
           #legend.text = element_text(size = legend_text_size)) +
     labs(title="Comparison of encoding schemes for drug descriptors", x="Type", y="Spearman's correlation coefficient") + 
        stat_compare_means(method = "t.test",
                       size = p_value_size,
                       label.y = 0.95,
                       #label =  "p.signif", 
                       #label.x = 1.5, 
                       #symnum.args = list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1),
                       #                   symbols = c("****", "***", "**", "*", " "))
      )

figure_annotated = annotate_figure(g,
                                  #top = text_grob(paste(), size = overall_title_size),
                                  #left = text_grob(paste(), rot = 90, size = common_axis_text_y_size),
                                  #bottom = text_grob(paste(), size = common_axis_text_x_size),
                                  fig.lab = letters[4],
                                  fig.lab.size = figure_label_size,
                                  fig.lab.face = figure_label_face
                                  )

# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7d.png"), width = 800, height = 800)
print(figure_annotated)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7d.eps"), width = 25, height = 25)
figure_annotated
dev.off()
```

## Evaluating the model on clinical data
### Graphing (Fig. 7e)
```{r}
n_random_drug_combos = 99

# Load the scores. 
scores = read.csv(paste0(aim_dirs$Aim_2$Results, "Machine_learning/TCGA_PROGENy_ml_Combos_processed_full_scored.csv"), row.names = 1)
# Load the ML pData. 
pData = read.csv(filename_generator(data_dir = data_dir, data_source = "TCGA", data_set = "PROGENy", data_type = "ML", processing_stage = "Processed", input_or_output = "Input", data_subset_type = "Full", additional_info = "Combos", extension = ".csv"))
# Attach the scores.
pData$Score = scores[,1]

# Load the original pData.
pData_combos = readRDS(filename_generator(data_dir, data_source = "TCGA", data_type = "pData", extension = ".rds", processing_stage = "Processed"))

# Find the samples that are CRs/PRs.
responders = pData_combos %>% dplyr::filter(Response %in% c("Complete Response", "Partial Response")) %>% .[grep(.$Drugs, "_"),] %>% .$Sample %>% as.character # The last part filters out single-drug treatments.
# Subset pData and scores.
pData_responders = pData %>% dplyr::filter(Sample %in% responders)

# Create the data frame.
n_samples = pData_responders$Sample %>% unique %>% length
dat = data.frame(
  Sample = pData_responders$Sample,
  Drugs = pData_responders$Drugs,
  Score = pData_responders$Score,
  Treatment = rep(c("Y", rep("N", n_random_drug_combos)), n_samples)  # n_random_drug_combos from section "Putting it together."
) %>% 
  dplyr::group_by(Sample) %>% 
  dplyr::arrange(Score, .by_group = T)

# Get the rank (percentile) of each treatment combo. 
percentile = ecdf(1:(n_random_drug_combos + 1))
percentiles = which(dat$Treatment=="Y") %>% mod(100) %>% percentile
percentiles = (1 - percentiles) * 100 %>% round(digits = 0) # Actual distribution of percentiles. 

# Create a random distribution. 
# Sample n from a uniform distribution of 1:m 10000 times, where n = number of samples and m = number of random combinations + 1 (number of actual combinations.)
sample_size = length(percentiles)
n_combos_total = n_random_drug_combos + 1
n_samples = 10000

rand_dist = c()
for(i in 1:n_samples) {
  # Sample from a uniform distribution.
  set.seed(i)
  sample = extraDistr::rdunif(sample_size, 1, n_combos_total)
  mu = mean(sample)
  
  rand_dist = c(rand_dist, mu)
}
dat = data.frame(Percentile = rand_dist, Group = rep("Random", length(rand_dist)))
p_val = 1 - pnorm(mean(percentiles), mean = mean(rand_dist), sd = sd(rand_dist))
t.test(rand_dist, mu = mean(percentiles), alternative = "less")

# Graph.
histogram = dat %>% ggplot(aes(x = Percentile, color = Group)) + 
  geom_histogram(aes(fill=Group), position="identity") + 
  scale_color_manual(values=bar_colors) +  
  scale_fill_manual(values=bar_colors) + 
  theme_classic() + 
  theme(text=element_text(family = font, face = "bold"),
        plot.title = element_text(size = title_size, face = title_face, margin = title_margins),
        axis.line = element_line(size = axis_line_size),
        axis.text.x = element_text(color = axis_text_x_color, size = axis_text_x_size, hjust = .5, vjust = .5, face = axis_text_x_face),
        axis.title.x = element_text(color = axis_title_x_color, size = axis_title_x_size, hjust = .5, vjust = .5, face = axis_title_x_face),
        axis.text.y = element_text(color = axis_text_y_color, size = axis_text_y_size, hjust = .5, vjust = .5, face = axis_text_y_face),
        axis.title.y = element_text(color = axis_title_y_color, size = axis_title_y_size, hjust = .5, vjust = .5, face = axis_title_y_face, margin = margin(t = axis_title_y_margin_t, r = axis_title_y_margin_r, b = axis_title_y_margin_b, l = axis_title_y_margin_l)),
        axis.ticks.length.y = unit(tick_size, "pt"), # https://stackoverflow.com/a/41251643
        axis.ticks = element_line(size = axis_thickness), # We want tick size to be the same as the axis size. 
        legend.position = "none"
        ) +
           #legend.title = element_text(size = legend_title_size),
           #legend.text = element_text(size = legend_text_size)) +
        labs(title="Distribution of percentiles under the null hypothesis", x="Percentile", y="Count") +
  geom_vline(xintercept = mean(rand_dist), linetype="dashed") + 
  annotate(
    "label", label = "paste(\"Average percentile under null = 50.5\")", parse = T,
    x = mean(rand_dist), y = 1000
  ) + 
  geom_vline(xintercept = mean(percentiles), linetype="dashed") + 
  annotate(
    "label", label = "paste(\"Average percentile of actual treatments = 71.6\")", parse = T,
    x = mean(percentiles), y = 1000
  ) + 
  geom_text(x=65, y=2000, label="T-test, p < 2.2e-16", color = axis_text_x_color, size = (p_value_size - 2))

figure_annotated = annotate_figure(histogram,
                                  #top = text_grob(paste(), size = overall_title_size),
                                  #left = text_grob(paste(), rot = 90, size = common_axis_text_y_size),
                                  #bottom = text_grob(paste(), size = common_axis_text_x_size),
                                  fig.lab = letters[5],
                                  fig.lab.size = figure_label_size,
                                  fig.lab.face = figure_label_face
                                  )

# Save to PNG (test).
png(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7e.png"), width = 2200, height = 800)
print(figure_annotated)
dev.off()

# Save to EPS.
setEPS()
postscript(paste0(aim_dirs$Aim_2$Results, "Figures/Figure7e.eps"), width = 25, height = 25)
figure_annotated
dev.off()
```

## Evaluating the model on the O'Neil set
```{r}
# Set the file names.
# Input filenames.
ds = "ONeil"
input_filenames = list()
input_filenames[["ONeil"]] = paste0(folder_generator(data_dir, data_source = "ONeil", data_type = "ML", processing_stage = "Processed"), "ONeil_PROGENy_DrugComb_ml_S1_processed_full.csv")

# Check if the input and output file exist.
# If the input file does not, or if both the output files do, skip.
input_filename = input_filenames[[ds]]
data_source = ds %>% regexPipes::gsub("_.+$", "")
folder = paste0(folder_generator(data_dir, data_source = data_source, data_type = "ML", processing_stage = "Processed"))
output_filename = input_filename %>% regexPipes::gsub(folder, "") %>% regexPipes::gsub("\\.csv", "_scored.csv")
output_file_path = paste0(aim_dirs$Aim_2$Results, output_filename)

# Load the O'Neil scores.
ONeil_scores = read.csv(output_file_path) %>% .[,2]

# Load the O'Neil pData (actually the ML data).
ONeil_pData = read.csv(input_filename)
# Merge the scores.
ONeil_pData$scores = ONeil_scores
ONeil_pData = ONeil_pData %>% relocate(scores, .after = block_id)
# Merge duplicated rows (cell line x drug combos.)
ONeil_pData = ONeil_pData %>% dplyr::select(drugs, cell_line_name, scores, css_ri) %>% group_by(drugs, cell_line_name) %>% mutate_each(funs(max)) %>% distinct # first entry only: ONeil_pData %>% dplyr::select(drugs, cell_line_name, scores, css_ri) %>% distinct(drugs, cell_line_name, .keep_all = T) # by average: ONeil_pData %>% dplyr::select(drugs, cell_line_name, scores, css_ri) %>% group_by(drugs, cell_line_name) %>% mutate_each(funs(mean)) %>% distinct

# Get the Spearman correlation coefficient for each cell line.
cell_lines = ONeil_pData$cell_line_name %>% unique
ccs = c()
for(cl in cell_lines) {
  # Get all the entries corresponding to that cell line.
  ONeil_pData_i = ONeil_pData %>% dplyr::filter(cell_line_name == cl)
  
  # Calculate correlation.
  spearman_cor = cor(ONeil_pData_i$css_ri, ONeil_pData_i$scores, method = "spearman")
  
  # Add to the vector.
  ccs = c(ccs, spearman_cor)
}
```

# Supplemental material
## Supplemental figure: Drug-target data: exploratory data analysis
```{r}
# Get the landmark L1000 genes.
LINCS_geneinfo_beta = read.csv(paste0(folder_generator(data_dir, data_source = "CMAP-LINCS", data_type = "Metadata", processing_stage = "Raw"), "geneinfo_beta.csv"))
predictor_genes = LINCS_geneinfo_beta %>% dplyr::filter(feature_space=="landmark") %>% dplyr::select(gene_symbol) %>% unlist %>% as.character

# Get all possible drug targets for 1) KEGG/DB, 2) CTD, 3) TRRUST.
# For each data set, if the number of drugs with no targets within the L1000 > 50%, then change the L1000 to be the union of the L1000 and all the drug targets for that data set. 
data_types = c("DrugBank_KEGG", "CTD", "TRRUST")
data_type_drug_target_table = data.frame(
  DataSet = character(),
  PercentDrugsAboveCutoff = numeric(),
  stringsAsFactors = F
)
drug_target_table_list = list()
for(data_type in data_types) {
  # Load the data set.
  data_set = readRDS(filename_generator(data_dir, data_source = "GDSC", data_type = "Drugs", data_set = data_type, extension = ".rds", additional_info = "target_data", processing_stage = "Processed"))
  
  # For each drug, get the targets of that drug. 
  drug_target_table = data.frame(
    Drug = character(),
    NumTargets = integer(),
    NumTargetsInL1000 = integer(),
    PercentTargetsInL1000 = numeric(),
    stringsAsFactors = F
  )
  drugs = names(data_set)
  percent_targets_cutoff = 0.5 # 50%
  for(drug in drugs) {
    targets = data_set[[drug]]$Target %>% unique %>% as.character
    num_targets = length(targets)
    num_targets_in_L1000 = length(intersect(targets, predictor_genes))
    percent_targets_in_L1000 = num_targets_in_L1000  / length(targets) %>% round(2)
    
    drug_target_table = rbind(drug_target_table, data.frame(
      Drug = drug,
      NumTargets = num_targets,
      NumTargetsInL1000 = num_targets_in_L1000,
      PercentTargetsInL1000 = percent_targets_in_L1000
    ))
  }
  drug_target_table$PercentTargetsInL1000 = drug_target_table$PercentTargetsInL1000 %>% round(2)
  drug_target_table$NumTargets = drug_target_table$NumTargets %>% as.integer
  drug_target_table_list[[data_type]] = drug_target_table
  
  percent_drugs_above_cutoff = drug_target_table %>% dplyr::filter(PercentTargetsInL1000 > percent_targets_cutoff) %>% nrow %>% divide_by(length(drug_target_table$Drug)) %>% round(2) # magrittr
  data_type_drug_target_table = rbind(data_type_drug_target_table, data.frame(
    DataSet = data_type,
    PercentDrugsAboveCutoff = percent_drugs_above_cutoff
  ))
}
data_type_drug_target_table$PercentDrugsAboveCutoff = data_type_drug_target_table$PercentDrugsAboveCutoff %>% round(2)
```

